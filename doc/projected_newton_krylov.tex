% SIAM Article Template
\documentclass[]{siamart1116}
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{bm} 
\usepackage{float}
\usepackage{graphicx}
\DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\numberwithin{theorem}{section}
\numberwithin{equation}{section}
\usepackage[english]{babel}
\newtheorem{lem}{Lemma}

% Declare title and authors, without \thanks
\newcommand{\TheTitle}{Projected Quasi-Newton-Krylov for Reaction-Diffusion Equations} 
\newcommand{\TheAuthors}{T. Valentinova Avramova, P.C. Africa, A. Scotti, L. Formaggia and C. de Falco}

% Sets running headers as well as PDF title and authors
\headers{\TheTitle}{\TheAuthors}

\title{{\TheTitle}\thanks{Submitted to the editors DATE.
\funding{FONDI DA RINGRAZIARE}}}

\newcommand{\MOXaddress}{MOX - Modeling and Scientific Computing, Dipartimento di Matematica, Politecnico di Milano, P.zza L. Da Vinci 32, 20133 - Milano - ITALY}
% Authors: full names plus addresses.
\author{
  Temenuzhka Valentinova Avramova\thanks{\MOXaddress (\email{temenuzhka.avramova@mail.polimi.it})}
 \and
  Pasquale Claudio Africa\thanks{\MOXaddress (\email{pasqualeclaudio.africa@polimi.it}).}
  \and
  Anna Scotti\thanks{\MOXaddress (\email{anna.scotti@polimi.it}).}
  \and
  Carlo de Falco
  \thanks{\MOXaddress (\email{carlo.defalco@polimi.it}).}
  \and
  Luca Formaggia
  \thanks{\MOXaddress (\email{luca.formaggia@polimi.it}).}
}

\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}


% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={\TheTitle},
  pdfauthor={\TheAuthors}
}
\fi

% FundRef data to be entered by SIAM
%<funding-group>
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>


\begin{document}
\tableofcontents
\maketitle

% REQUIRED
\begin{abstract}
  We study the application of a projected quasi-Newton-Krylov method to a benchmark parabolic Reaction Diffusion equation.
\end{abstract}

% REQUIRED
\begin{keywords}
  AMR, quadtrees, parallel computing, quasi-Newton, Inexact Newton, Newton-Krylov
\end{keywords}

% REQUIRED
\begin{AMS}
  68Q25, 68R10, 68U05
\end{AMS}


\section{Introduction}
Let's consider this nonlinear problem 
\begin{equation*}
\begin{cases}
F(\textbf{x}) = 0\\\textbf{x} \in \mathbb{R}^n,
\end{cases}
\end{equation*}
where $F: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is continuously differentiable.\\
Nonlinear equations cannot be solved analytically in general. In this case, therefore, the
solutions of the equations must be approached using \textit{iterative methods}.
The method that we are going to analyse is the Newton's method, which is a particular case of \textit{fixed point iteration method}. \\
In particular, the Newton sequence is
\begin{equation}
{\textbf{x}}_{k+1} = {\textbf{x}}_{k} - F'({\textbf{x}}_{k})^{-1} F({\textbf{x}}_{k}), 
\label{Newton_it}
\end{equation}
where we start from a initial guess ${x}_{0}$. We can view \eqref{Newton_it} as the two-term Taylor expansion in which we impose $F({\textbf{x}}_{k+1})=0$.\\

Let's introduce some assumption and definition useful for the convergence's theory.\\
\textbf{Standard assumptions:} 
\begin{itemize}
	\item $F(\textbf{x}) = 0$ has a solution ${x}^{*}$;
	\item $F'(\textbf{x}): \Omega \rightarrow \mathbb{R}^{n}$ is Lipschitz continuous;
	\item $F'({\textbf{x}}^{*})$ is non singular.
\end{itemize}
\noindent \textbf{Convergence's definitions}: \textit{ Let $\alpha \in \mathbb{R}^{n}$ and ${\textbf{x}}_{k} \in \mathbb{R}^{n}$, $k = 0,1,2,...$ Then ${\textbf{x}}_{k}$ is said:}
	
	\begin{itemize}
		\item \textit{ \textbf{q-linearly convergent} if there exists a constant $C \in (0,1)$ and an integer $m$ such that for all $k\geq m$ 
		\begin{equation*}
		||{\textbf{x}}_{k+1}-\alpha|| \leq C||{\textbf{x}}_{k}-\alpha|| .
		\end{equation*}	}
	    \item \textit{ \textbf{q-superlinearly convergent} if there exists a sequence ${{C}_{k}}$ convergent to 0 such that
		\begin{equation*}
		||{\textbf{x}}_{k+1}-\alpha|| \leq C_k||{\textbf{x}}_{k}-\alpha|| .
		\end{equation*}}
	
	\item \textit{ \textbf{convergent sequence of q-order p} $(p > 1)$ if there exists a
		constant $C$ and an integer $m > 0$ such that for all $k \geq m$
		\begin{equation*}
		||{\textbf{x}}_{k+1}-\alpha|| \leq C{||{\textbf{x}}_{k}-\alpha||}^{p} .
		\end{equation*}}
\end{itemize}

\begin{theorem}
	\label{converg}
 Let the standard assumptions hold. Then there is a $\delta > 0$ such that, if the initial guess ${\textbf{x}}_{0} \in \mathit{B(\delta)}$, then the Newton interation \eqref{Newton_it} converges q-quadratically to ${\textbf{x}}^{*}$, that is $||{\textbf{e}}_{k+ 1}|| \leq C ||{\textbf{e}}_{k}||$, for some $ C > 0 $ and with $ ||{\textbf{e}}_{k}|| = ||{\textbf{x}}_{k} - \textbf{x}_{ex}|| $ .
\end{theorem}
Since the initial guess needs to be "sufficiently near" to the solution, the convergence of the Newton's method is local.\\

A practical problem is that, in general, we don't have the analytical solution $\textbf{x}_{ex}$, so we can not calculate the error $||{\textbf{e}}_{k}|| = ||{\textbf{x}}_{k} - \textbf{x}_{ex}|| $. Therefore, we have to find another estimation of the error that can be used as an \textit{arrest criterion} of the iterate method.
For example, it is used the \textit{relative nonlinear residual} $||F(\textbf{x})||/||F({\textbf{x}}_{0})||$, that is a good indicator of size of the error, but only when $F'({\textbf{x}}^{*})$ is well conditioned. Indeed we have:
\begin{lem}
 Let the standard assumptions hold and $\delta > 0$ be small enough. Then for all $\textbf{x} \in \mathit{B(\delta)}$
\end{lem}

\begin{equation*}
\frac{||\textbf{e}||}{4 ||{\textbf{e}}_{0}|| \mathit{k}(F'({\textbf{x}}^{*}))} \leq \frac{||F(\textbf{x})||}{||F({\textbf{x}}_{0})|} \leq \frac{4 \mathit{k}(F'({\textbf{x}}^{*}))||\textbf{e}||}{||{\textbf{e}}_{0}||}
\end{equation*}
\textit{where $\mathit{k}(F'({\textbf{x}}^{*})) =||F'({\textbf{x}}^{*})||$ $ ||{F'({\textbf{x}}^{*})}^{-1}|| $ is the condition number of $F'({\textbf{x}}^{*})$ relative to the norm $||\cdot||$.}

\noindent Another way to decide whether to terminate is to look at the Newton \textit{step}
\begin{equation*}
{\textbf{s}}_{k+1} ={\textbf{x}}_{k+1} - {\textbf{x}}_{k}= -{F'({\textbf{x}}_{k})}^{-1}F({\textbf{x}}_{k}),
\end{equation*}
and except $ \textbf{x}_{k+1} $ as good approximation of the solution when $||{\textbf{s}}_{k+1}||$ is sufficiently small. This criterion is based on Theorem \ref{converg} which implies that 
\begin{equation*}
||{\textbf{e}}_{k+1}|| = ||{\textbf{s}}_{k+1}|| + \mathcal{O}({||{\textbf{e}}_{k+1}||}^{2}).
\end{equation*}
Hence, near the solution $\textbf{s}$ and $\textbf{e}$ are essentially the same size.\\ 

Sometimes it is too expensive from a computational point of view to evaluate $F'(\textbf{x})$ at each step and sometimes, actually, there is no need to be so precise because, for example, we are too far from the solution. And also we would like to have a global convergence, instead just of a local one. There are many techniques that are done for these requirements, and in the next session we illustrate some of them.
\section{Review of Variants of the Newton Method}
This section will regard mainly different ways to approximate ${F'}^{-1}$, but first of all we want to give a theoretical result about inaccuracy. Suppose that $F + \epsilon$ and $F' + \Delta$ are used instead of $F$ and $F'$ in the iteration, then we have the following result.
\begin{theorem}
	\label{accuracy}
	Let the standard assumptions hold. Then there are $K>0$, $\delta>0$ and $\delta_1>0$ such that if $\textbf{x}_{k} \in \mathit{B}(\delta)$ and $||\Delta(\textbf{x}_{k}) ||<\delta_1$ then 
 \begin{equation*}
 \textbf{x}_{k+1}=\textbf{x}_{k} - (F'(\textbf{x}_k)+\Delta(\textbf{x}_k))^{-1}(F(\textbf{x}_k)+\epsilon(\textbf{x}_k))
 \end{equation*}
	is defined (i.e.,$F'(\textbf{x}_k)+\Delta(\textbf{x}_k)$ is nonsingular) and satisfies 
\begin{equation*}
||{\textbf{e}}_{k+1}|| =K(||{\textbf{e}}_{k}||^2 + ||\Delta(\textbf{x}_n)|||\textbf{e}_k ||+||\epsilon(\textbf{x}_k) || ).
\end{equation*}
	
\end{theorem} 
As we observe, it can happen that the convergence is not quadratical anymore. 
\subsection{Quasi-Newton methods} 
Under this name we have all the Newton methods that don't calculate the real value of ${F'(\textbf{x})}^{-1}$, but use approximations. The price for such an approximation is that the nonlinear iteration converges more slowly; i.e., more nonlinear iterations are needed to solve the problem. However, the overall cost of solving is usually smaller, because the computation of the Newton step is less expensive.
Therefore, we are obligated to use a quasi-Newton method when is unavailable to have the exact expression of ${F'(\textbf{x})}^{-1}$ or is too expensive to compute it at every iteration. \\
Let's illustrate same of this methods. \\

\subsubsection{Chord method or modified Newton method} In this case the Newton iteration is given by
\begin{equation*}
{\textbf{x}}_{k+1} = {\textbf{x}}_{k} - F'({\textbf{x}}_{0})^{-1} F({\textbf{x}}_{k}) .
\end{equation*}
Let's suppose again that the standard assumptions hold. Assuming that the initial iteration is near enough to the solution ${\textbf{x}}^{*}$, the convergence of the chord iteration is q-linear. Indeed, this comes from Theorem \ref{accuracy}, noticing that $\epsilon(\textbf{x}_k) = 0$ and $||\Delta(\textbf{x}_k)|| = \mathcal{O}(||\textbf{e}_0||)$.\\
 In general, we can also update $F'(\textbf{x})^{-1}$ after $m$ iterations and not use $F'({\textbf{x}}_{0})^{-1}$ always.\\
A general way to implement chord-type methods is 
\begin{equation*}
{\textbf{x}}_{k+1} = {\textbf{x}}_{k} - {A}^{-1} F({\textbf{x}}_{k}), 
\end{equation*}
where $A \approx F'({\textbf{x}}^{*})$. Also in this case, if we have a good guess ${\textbf{x}}_{0}$ and $A$ is a good approximation of $ F'({\textbf{x}}^{*})$, then we have a \textit{q-linear} convergence. \\

\subsubsection{Shamanskii method} It consists in a alternation of a Newton step with a sequence of chord steps and leads to a class of \textit{high-order methods}, that is, methods that converge q-superlinearly with q-order larger that 2. \\
We can describe the transition from ${\textbf{x}}_{k}$ to ${\textbf{x}}_{k+1}$ by
\begin{gather*}
{\textbf{y}}_{1} = {\textbf{x}}_{k} -{F'({\textbf{x}}_{k})}^{-1}F({\textbf{x}}_{k}),\\
{\textbf{\textbf{y}}}_{j+1} = {\textbf{y}}_{j} -{F'({\textbf{x}}_{k})}^{-1}F({\textbf{y}}_{j}) \; \; \; \; \; for\; 1\leq j \leq m-1, \\
{\textbf{x}}_{k+1} = {\textbf{y}}_{m}
\end{gather*}

Note that for $m=1$ it is Newton's method and for $m=\infty$ it is the simplest chord method.
\begin{theorem}
	Let the standard assumptions hold and let $m \geq 1$ be given. Then there are $K_{S}>0$ and $\delta>0$ such that if $\textbf{x}_{0} \in \mathit{B(\delta)}$, the Shamankii iterates converge q-superlinearly to $\textbf{x}^{*}$ with q-order $m+1$ and 
	\begin{equation*}
	||\textbf{e}_{k+1} ||\leq K_{S} ||\textbf{e}_{k} ||^{m+1}.
	\end{equation*}
\end{theorem}
The advantage of the Shamanskii method over Newton's method is that hight q-order can be optained with far fewer Jacobian evaluations or factorizations.\\

\subsubsection{Difference approximations of the Jacobian matrix}. Another possibility consists of replacing $F'(\textbf{x}_{k})$ with an approximation through \textit{n}-dimensional
differences of the form
\begin{equation*}
(F'^{k}_{h})_{j} = \frac{F(\textbf{x}_{k} + h^{k}_{j} \textbf{e}_{j}) - F(\textbf{x}_{k})}{h_{j}^{k}},\;\;  \forall k \geq 0,
\end{equation*}
	
where $\textbf{e}_j$ is the j-th vector of the canonical basis of $\mathbb{R}^n$ and $h_j^k>0$ are
increments to be suitably chosen at each step $k$ of the iteration.\\
 \textit{Convergence's result}. Under the standards assumptions, a initial guess "sufficiently near" to the solution and bounded $ h^{k}_{j}$ for $j=1,...,n$, then the sequence 
 \begin{equation}
   \textbf{x}_{k+1}=\textbf{x}_k -[F'^{k}_{h}]^{-1} F(\textbf{x}_k)
   \label{iter_conv_result}
 \end{equation}
 converges \textit{linearly} to $\textbf{x}^*$. Moreover, if there exists a
 positive constant $C$ such that $\max_{j}|h^{k}_{j}| \leq C ||\textbf{x}_k - \textbf{x}^*||$	or, equivalently,
 there exists a positive constant $c$ such that $\max_{j}|h^{k}_{j}| \leq c ||F(\textbf{x}_k)||$, then
 the sequence \eqref{iter_conv_result}, is convergent \textit{quadratically}. But we should pay attention also not to choose $h_j^k$ too small in order to avoid big errors of truncation. 
 \\
 
\subsubsection{Secant method}This case is done for single equations $f'(x) = 0$ and the derivative is approximated using a finite difference with the most recent two iterations. 
\begin{equation}
{x}_{k+1} = {x}_{k} -\frac{({x}_{k}-{x}_{k-1}) f({x}_{k})}{f({x}_{k})-f({x}_{k-1})} 
\label{it_secant_method}
\end{equation} 
For the computation of ${x}_{1}$, one option is to set ${x}_{-1} = 0.99{x}_{0}$. If the standard assumptions hold and $ x_{-1} $ and $ x_0  $ are sufficiently near the solution, Theorem \ref{accuracy}, with $\epsilon = 0$ and $||\Delta(x_k)||=\mathcal{O}(||e_{k-1}||)$, implies that the iteration converges\textit{ q-superlinearly}.\\
 \subsubsection{Broyden's method}This is a version of secant method for higher dimensions than 1.\\
In a multidimentional case the equation \eqref{it_secant_method} has no sense, because we can't divide by a vector, so we ask that ${B}_{k}$, the current approximation od $F'(\textbf{x})$, satisfies the secant equations 
\begin{equation*}
{B}_{k}({\textbf{x}}_{k} - {\textbf{x}}_{k-1}) = F({\textbf{x}}_{k}) - F({\textbf{x}}_{k-1}).  
\end{equation*} 
A wide variety of methods, that satisfy the secant equations, have been designed to preserve such properties of the Jacobian as the sparsity patter or symmetry. In the case of Broyden's method, we have
\begin{equation*}
{\textbf{x}}_{k+1} = {\textbf{x}}_{k} - {\lambda}_{k}{B}_{k}^{-1}F({\textbf{x}}_{k}).  
\end{equation*} 
where ${\lambda}_{k}$ is the step length for Broyden direction
\begin{equation*}
{\textbf{d}}_{k} = -{B}_{k}^{-1}F({\textbf{x}}_{k})  
\end{equation*} 
After the computation of ${\textbf{x}}_{k+1}$, ${B}_{k}$ is updated 
\begin{equation*}
{B}_{k+1} = {B}_{k} + \frac{(\textbf{y} - {B}_{k}\textbf{s}){\textbf{s}}^{T}}{{\textbf{s}}^{T}\textbf{s}}  
\end{equation*} 
with $\textbf{y} = F({\textbf{x}}_{k+1}) - F({\textbf{x}}_{k})$ and $ \textbf{s} = {\textbf{x}}_{k+1}-{\textbf{x}}_{k} = {\lambda}_{k}{\textbf{d}}_{k} $.\\
Broyden's method does not guarantee that the approximate Newton direction will be a descent direction for $||F||$  (the same can happen also with the secant method).
Under hypothesis of standard assumptions and both ${\textbf{x}}_{0}$ and ${B}_{0}$ are enough good approximations of $ \textbf{x}^* $ and $ F'(\textbf{x}^*) $, the convergence theory for Broyden's method is \textit{q-superlinear}. So it is only local and, therefore, less satisfactory than that for the Newton and Newton-Krylov methods (we will see in the next subsections). Moreover the line search cannot be proved to compensate for poor initial iterate, because it can work for sure only with a good approximation of $F'({\textbf{x}}_{k})$.
\\
 \subsubsection{Approximation of a sparse Jacobian}
 In general the advantages of approximating $ J_k $, the Jacobian matrix, instead of its inverse $ J_k^{-1} $, are more evident when we have a sparse Jacobian with known nonzero positions. Indeed the inverse matrix could not be sparse and so it could need a storage much bigger and, if we know the pattern of the matrix, we can directly set these values to zero and have less conditions to solve. By the way, this scenario is typical for differential nonlinear problems, for which the Jacobian is sparse with known pattern. \\
  Let's recall the general nonlinear problem $ F(\textbf{x}) = 0 $ and its iterative step:
 \begin{eqnarray*}
 J_k \textbf{s}_k = - F_k, \\
\textbf{x}_{k+1} = \textbf{x}_k + \lambda_k \textbf{s}_k,
 \end{eqnarray*}
 with $ \lambda_k $ a positive scalar, that reduces the length of the step to achieve stability, and $ J_k  $ approximation of the Jacobian matrix. 
 In order to find $ J_k $, we use first primary conditions, that impose to $ J_k $ to predict the same changes along the direction $ \textbf{s}_k $ as $ F_k $ does, therefore 
 \begin{equation}
 \label{prim}
 J_{k+1} = J_k + \frac{ [ F_{k+1} - (1-t_k) F_k ] \textbf{s}_k^T}{t_k \textbf{s}_k^T \textbf{s}_k}.
 \end{equation}
 So we need to find conditions for the $ n^2 -n $ unknowns that remain. These are called secondary conditions and they can be chosen differently, for example, $ J_k $ can be forced to give the same variations that $ F_k $ has along all orthogonal directions on $ \textbf{s}_k $.\\
  As we can see, the logic is the one used Broyden's method, but, here, it has to be adapted to the sparse case, let's see how.\\
  The $ i $th row $ g_k^{(i)} $ of $ J_k $ is an approximation to the gradient of the $ i $th function component $ F_k^{(i)} $. If $ n-r_i $ components of $g_k^{(i)} $ are known constants, we impose first the condition that these components shall remain unchanged in the Jacobian revision, and then we implement the other conditions on the basis of the remaining $ r_i $ coordinate directions.
  The vectors $ \hat{\textbf{s}}_k $ and $ \bar{g}^{(i)} $ are introduced, where the first one is a column vector that is the same as $ \textbf{s}_k $, but setting  $\textbf{s}_k^{(j)} $ to zero whenever the corresponding element of $ g_k^{(i)} $ is a known constant. Indeed, $ \bar{g}^{(i)} $ is a row vector that is as $ g_k^{(i)} $, but setting its unknown elements to zero. 
  The primary conditions becomes
\begin{equation}
\label{firstcond}
	t_k g_{k+1}^{(i)} \hat{\textbf{s}}_k = F_{k+1}^{(i)} -F_{k}^{(i)} - t_k \bar{g}^{(i)} \textbf{s}_k, \; \; \; i = 1,2,\dots,n.
\end{equation}
  This is identical to the primary condition $ t_k J_{k+1} \textbf{s}_k = F_{k+1} - F_k$, because $ g_{k+1}^{(i)} \hat{\textbf{s}}_k + \bar{g}^{(i)} \textbf{s}_k = g_{k+1}^{(i)} \textbf{s}_k$. \\
  The secondary condition is obtained in a similar way by restricting the previous secondary condition to the $ r_i $-space corresponding to the unknown elements of $ g_i $, that is:
  \begin{equation}
  \label{seccond}
  g_{k+1}^{(i)} \hat{q} = g_k^{(i)} \hat{q}, \; \; \; i = 1, 2, \dots, n,
  \end{equation}
  where $ \hat{q} $ satisfies $ \hat{\textbf{s}}_{k}^{T} \hat{q} = 0 $.
  It can be verified that \eqref{firstcond} and \eqref{seccond} are satisfied by the exact row-by-row analogue of \eqref{prim}, that is 
  \begin{equation*}
  g_{k+1}^{(i)} = g_{k}^{(i)} + \frac{ [ F_{k+1}^{(i)} - (1-t_k) F_k^{(i)} ] \hat{\textbf{s}}_k^T}{t_k \hat{\textbf{s}}_k^T \hat{\textbf{s}}_k}.
  \end{equation*}
  All this method is stated in \cite{Schubert} by L.K. Schubert and it is also shown that, when the dimension $ n $ increases, this approach is much better than classical Broyden's method for a sparse Jacobian.
  
\section{Inexact Newton Method} \label{inexact_Newton_method} Rather than approximate the Jacobian, one could instead solve the equation for the Newton step approximately. An inexact Newton method uses as a step a vector $\textbf{s}$ that satisfies the inexact Newton condition
\begin{equation}
||F'(\textbf{x}_k)\textbf{s}+F(\textbf{x}_k)|| \leq \eta ||F(\textbf{x}_k)||.
\label{inexact_newton}
\end{equation}
The parameter $\eta$ is called \textbf{forcing term}. Away from the solution $\textbf{x}^*$, $F$ and its local linear model may disagree considerably at a step that closely approximates the Newton step. So choosing $\eta$ too small may lead to \textit{oversolving} the Newton equation. Therefore far from the solution, a less accurate approximation of the Newton step may be both cheaper and more effective. So, the idea is to choose a forcing term that becomes smaller when the iteration are closer to the solution. And what about the convergence?
\begin{theorem} \label{convergenza_IN}
	Let the standard assumptions hold. Then there are $\delta$ and $\bar{\eta}$ such that, if $\textbf{x}_0 \in \mathit{B}(\delta)$, ${\eta_n}\subset [0,\bar{\eta}]$, then the inexact Newton iteration
	\begin{equation*}
	\textbf{x}_{k+1} = \textbf{x}_k + \textbf{s}_k
	\end{equation*}
	where
   \begin{equation*}
   ||F'(\textbf{x}_k)\textbf{s}_k+F(\textbf{x}_k)|| \leq \eta_k ||F(\textbf{x}_k)||,
   \end{equation*}
	converges q-linearly to $\textbf{x}^*$. Moreover, 
	\begin{itemize}
		\item if $\eta_n \rightarrow 0$, the convergence is q-superlinear, and 
		\item if $\eta_k \leq K_{\eta} ||F(\textbf{x}_k) ||^p$ for some $K_{\eta}>0$, the convergence is q-superlinear with q-order $1+p$.
	\end{itemize}
	
\end{theorem}
In literature, in particular \cite{Forcingterm}, we can find the following choices of $\eta$. \\
\noindent \textbf{Choice 1.} Given $\eta_0 \in [0,1)$, choose
\begin{equation}
\eta_k=\frac{||F(\textbf{x}_k) - F(\textbf{x}_{k-1}) - F'(\textbf{x}_{k-1})\textbf{s}_{k-1} ||}{||F(\textbf{x}_{k-1})||},\;\;\;\;\; k=1,2,3...
\label{Choice1.1}
\end{equation}
or
\begin{equation}
\eta_k=\frac{| ||F(\textbf{x}_k)|| - ||F(\textbf{x}_{k-1}) + F'(\textbf{x}_{k-1})\textbf{s}_{k-1} |||}{||F(\textbf{x}_{k-1})|| |},\;\;\;\;\; k=1,2,3...
\label{Choice1.2}
\end{equation}

 Note that $\eta_k$ given by \eqref{Choice1.1} and \eqref{Choice1.2} directly reflects the agreement between $F$ and its local linear model at the previous step. The choice \eqref{Choice1.2} may be more convenient to evaluate that \eqref{Choice1.1} in some circumstances. Since it is at least as small as \eqref{Choice1.1}, local convergence will be at least as fast as with \eqref{Choice1.1}.\\
 One possible way to obtain faster local convergence, while retaining the potential advantages of \eqref{Choice1.1} and \eqref{Choice1.2}, is to raise those expression to powers greater than one. \\
\noindent \textbf{Choice 2.} Given $\gamma \in [0,1]$, $\alpha \in (1,2]$, and $\eta_0 \in [0,1)$, choose
\begin{equation}
\eta_k=\gamma \left(\frac{||F(\textbf{x}_k)||}{||F(\textbf{x}_{k-1})||}\right)^{\alpha},\;\;\;\; k=1,2,3...
\label{Choice2}
\end{equation}
This choice does not directly reflect the agreement between $F$ and his local linear model. However, it can produce a little oversolving in practice.\\ 

In experiments it was observed that the forcing term choices occasionally become too small far away from a solution. There is a particular danger of the Choice 1 forcing terms becoming too small; indeed, a $\eta_k$ given by \eqref{Choice1.1} or \eqref{Choice1.2} can be undesirably small because of their very small step or coincidental very good agreement between F and its local linear model. For this reason there are \textbf{safegurads} that are intended to prevent the forcing term to become too small too soon.\\
\textit{Choice 1 safeguard}: Modify $\eta_k$ by $\eta_k = max  \{\eta_k, \eta_{k-1}^{\frac{(1+\sqrt{5})}{2}}\}$ whenever $\eta_{k-1}^\frac{(1+\sqrt{5})}{2} > 0.1$.\\
\textit{Choice 2 safeguard}: Modify $\eta_k$ by $\eta_k = max  \{\eta_k,\gamma  \eta_{k-1}^\alpha\}$ 
whenever $\gamma  \eta_{k-1}^\alpha > 0.1$.\\
As we can see, they are activated when the previous $ \eta $ was not so small as the new one tries to be, so it is more suspicious. \\
There is also another version of safeguard for the choice 2 in \cite{Kelley1}, and this is 
\begin{equation*}
\eta_k = min (\eta_{max}, max (\eta_k^C, 0.5 \tau_t/||F(\textbf{x}_k)||)),
\end{equation*}
with $\tau_t =  \tau_a + \tau_r||F(\textbf{x}_0)||$ and 
\begin{equation*}
\eta_k^C={
\begin{cases}
\eta_{max},\;\; k=0 \\
min(\eta_{max}, \eta_k),\;\; k>0,\;\; \gamma \eta_{k-1}^2\leq 0.1\\
min(\eta_{max}, max(\eta_k, \gamma \eta_{k-1}^2)),\;\; k>0,\;\; \gamma \eta_{k-1}^2> 0.1

\end{cases}}
\end{equation*}

Iterative methods for solving the equation for the Newton step would typically use \eqref{inexact_newton} as a termination criterion. In this case, the overall nonlinear solver is called a \textbf{Newton iterative method}, and they are named by the particular iterative method used for the linear equation. For example, there are Newton-Jacobi, Newton-SOR or Newton-Krylov.\\

\subsection{Newton-Krylov Methods} 
As we said before, for each iteration of the inexact Newton method, we have to solve a linear equation with an iterative method. Sometimes we refer to this linear iteration as an \textit{inner iteration}. Similarly, the nonlinear iteration is ofen called the \textit{outer iteration}.\\
The Newton-Krylov methods, as the name suggests, use Krylov subspace-based linear solver. It approximates the solution of a linear system $A\textbf{d}=\textbf{b}$ with a sum of the form 
\begin{equation*}
\textbf{d}_k=\textbf{d}_0+\sum_{j=0}^{k-1}\gamma_j A^j\textbf{r}_0,
\end{equation*}
where $\textbf{r}_0=\textbf{b}-A\textbf{d}_0$ and $\textbf{d}_0$ is the initial iterate. Since the goal is to approximate a Newton step, the most sensible initial iterate is $\textbf{d}_0=0$, because we have no priory knowledge of the direction, but, at least in the local phase of the iteration, expect it ot be small.\\
 We express this in compact form as $ \textbf{d}_k \in \mathcal{K}_k$, where the $k$th \textbf{Krylov subspace} is $\mathcal{K}_k = span  (\textbf{r}_0, A\textbf{r}_0, ...,A^{k-1}\textbf{r}_0)$.\\
 There are many Newton-Krylov methods and they differ in storage requirements, cost in evaluations of $F$ and robustness. If $A$ is symmetric and positive definite, the conjugate gradient (CG) method has better storage and convergence properties than others Newton-Krylov methods. If the matrix $A$ does not have this properties, then we can use two low-storage solvers, BiCGSTAB and TFQMR, but we have to be aware that this solvers can break down, because a division by zero can occur. An other option is GMRES, that is not low-storage solver (but it can become as we will see in the next section), and, when there is no convergence, instead of breaking down, there is just a stagnation in the iterations.
 \subsubsection{GMRES} 
 The $k$th Generalized Minimal Residual (GMRES) iterate is the solution of the linear least squares problem of minimizing 
 \begin{equation*}
 ||\textbf{b}-A\textbf{d}_k ||^2
 \end{equation*}
 over $\mathcal{K}_k$.\\
 An important property of the method, it that GMRES must accumulate the history of the linear iteration as an orthonormal basis for the Krylov subspaces. Therefore, for large problems, it can exhaust the available fast memory. For these cases, there is GMRES($m$), which restarts the iteration when the size of Krylov space exceeds $m$ vectors. \\
 Sometimes GMRES method, like other Krylov methods, is implemented as a \textit{matrix-free}. The reason is that only matrix-vector products, rather than details of the matrix, are needed to implement a Krylov method.\\
\noindent \textbf{Algorithm.} We want to solve the linear system 
\begin{equation*}
A \textbf{d}= \textbf{b},
\end{equation*}
using the $l_2$-orthogonal basis $V_k = \{\textbf{v}_1, ...,\textbf{v}_k \}$ of the space $\mathcal{K}_k$; in fact, we look for a solution of the type $\textbf{d}_k = \textbf{d}_0 + \textbf{z}_k$, where $\textbf{d}_0$ is an initial guess and $\textbf{z}_k \in \mathcal{K}_k$. Let's define the residual $\textbf{r}_k = \textbf{b}-A\textbf{d}_k$ and choose $\textbf{v}_1 = \frac{\textbf{r}_0}{||\textbf{r}_0||}$. We can write $\textbf{z}_k$ as a linear combination of $\{\textbf{v}_1, ...,\textbf{v}_k \}$, that is 
\begin{equation*}
\textbf{z}_k = V_k \textbf{y}_k .
\end{equation*}
We introduce $H$, the upper $k \times k$ Hessenberg matrix, that is $H \equiv V_k^T A V_k$. It is the matrix representation of $A_k$ in the basis $\{\textbf{v}_1, ...,\textbf{v}_k \}$, where $ A_k $ is the $ l_2 $-orthogonal projection of $ A $ in $ \mathcal{K}_k $. \\
Since we have $(\textbf{b} - A(\textbf{d}_k)) \perp \mathcal{K}_k$, that is $(\textbf{b} - A(\textbf{d}_0 + \textbf{z}\textbf{}_k)) \perp \mathcal{K}_k$ , then we know that $(\textbf{w},\textbf{r}_0 - A \textbf{z}_k) = 0 \; \; \forall \textbf{w} \in \mathcal{K}_k$. We can deduce that $V_k^T A \textbf{z}_k = V_k^T \textbf{r}_0$, and so $A \textbf{z}_k = \textbf{r}_0$ for $ k = n $, dimension of the system. That means that $ A \textbf{d}_n = \textbf{b} $. \\
 It is known that $\textbf{r}_0 =V_k \textbf{e}_1 ||\textbf{r}_0||$, with $\textbf{e}_1$ the unit vector $\textbf{e}_1 = (1, 0, ..., 0)^T$, then we deduce that $\textbf{y}_k = H_k^{-1} ||\textbf{r}_0||\textbf{e}_1$. 
\\

ALGORITHM 1 (\textit{Arnoldi's method}): Full orthogonalization method.\\
1. \textit{Start:} Choose $\textbf{d}_0$ and compute $\textbf{r}_0 = \textbf{b}- A \textbf{d}_0$ and $v_1 = \frac{\textbf{r}_0}{||\textbf{r}_0||}$. \\
2. \textit{Iterate:} For $j=1,2,...,k$ do:\\
\hspace*{1cm} $h_{i,j} = (A\textbf{v}_j,\textbf{v}_i),\;\; i = 1,2,...j,$\\ 
\hspace*{1cm} $\hat{\textbf{v}}_{j+1} = A\textbf{v}_j - \sum_{i=1}^{j}h_{i,j}\textbf{v}_i,$ \\
\hspace*{1cm} $ h_{j+1,j} = ||\hat{\textbf{v}}_{j+1}||,$\\
\hspace*{1cm} $\textbf{v}_{j+1} = \hat{\textbf{v}}_{j+1}/h_{j+1,j}$.\\
3. \textit{Form the solution:} \\
\hspace*{1cm} $\textbf{d}_k = \textbf{d}_0 + V_k \textbf{y}_k$  where  $\textbf{y}_k = H^{-1}_k ||\textbf{r}_0||\textbf{e}_1$ \\
The step 2 of the ALGORITHM 1 just uses the Gram-Schmidt method for computing an $l_2$-orthonormal basis $\{\textbf{v}_1, ..., \textbf{v}_k \}$. \\

Let's see now the GMRES algorithm that comes from the Arnoldi's one. After $k$ steps of Arnoldi's method, we have an $l_2$-orthonormal system $V_{k+1}$ and a $(k+1) \times k$ matrix $\bar{H}_k$, whose only non zero entries are the element $h_{ij}$ generated by the method. Thus the $\bar{H}_k$ is the same as $H_k$ except for an additional row, whose only nonzero element is $h_{k+1,k}$. We have this important relation:
\begin{equation}
\label{GMRES1}
AV_k=V_{k+1}\bar{H}_k.
\end{equation}
We would like to solve the least squares problem: 
\begin{equation}
\label{jmin}
\min_{\textbf{z} \in \mathcal{K}_k} ||\textbf{f}-A[\textbf{d}_0+\textbf{z}]|| = \min_{\textbf{z} \in \mathcal{K}_k}||\textbf{r}_0-A\textbf{z}||.
\end{equation}
We remember that $\textbf{z}=V_k\textbf{y}$, so we can see \eqref{jmin} as a function of $\textbf{y}$ to be minimized:
\begin{equation*}
J(\textbf{y})=||\beta \textbf{v}_1 -A V_k \textbf{y}||,
\end{equation*}
where we have $\beta = ||\textbf{r}_0||$. Using \ref{GMRES1} we obtain 
\begin{equation*}
J(\textbf{y})=||V_{k+1} ( \beta \textbf{v}_1 -\bar{H}_k V_k \textbf{y} ) ||.
\end{equation*}
Recalling the fact that $V_{k+1}$ is $l_2$-orthonormal and so that it preserve the norm, we see that 
\begin{equation}
\label{gmres2}
J(\textbf{y})=|| \beta \textbf{v}_1 -\bar{H}_k V_k \textbf{y} ||.
\end{equation}
In conclusion, the solution of the least squares problem \eqref{jmin} is given by 
\begin{equation*}
\textbf{d}_k = \textbf{d}_0 + V_k \textbf{y}_k,
\end{equation*}
where $\textbf{y}_k$ minimizes the function $J(\textbf{y})$, defined by \eqref{gmres2}, over $\textbf{y} \in \mathbb{R}^k$. \\

ALGORITHM 2 (\textit{GMRES}): Full orthogonalization method.\\
1. \textit{Start:} Choose $\textbf{d}_0$ and compute $\textbf{r}_0 = \textbf{b}- A \textbf{d}_0$ and $\textbf{v}_1 = \frac{\textbf{r}_0}{||\textbf{r}_0||}$. \\
2. \textit{Iterate:} For $j=1,2,...,k$ do:\\
\hspace*{1cm} $h_{i,j} = (A\textbf{v}_j,\textbf{v}_i),\;\; i = 1,2,...j,$\\ 
\hspace*{1cm} $\hat{\textbf{v}}_{j+1} = A\textbf{v}_j - \sum_{i=1}^{j}h_{i,j}\textbf{v}_i,$ \\
\hspace*{1cm} $ h_{j+1,j} = ||\hat{\textbf{v}}_{j+1}||,$\\
\hspace*{1cm} $\textbf{v}_{j+1} = \hat{\textbf{v}}_{j+1}/h_{j+1,j}$.\\
3. \textit{Form the approximate solution:} \\
\hspace*{1cm} $\textbf{d}_k = \textbf{d}_0 + V_k \textbf{y}_k$  where  $\textbf{y}_k$ minimizes \eqref{gmres2} \\
How to compute the step 3 of ALGORITHM 2 practically ? \\
We consider the $QR$-factorization of $\bar{H}_k$, so $Q_k \bar{H}_k = R_k$, with $Q_k$ a $(k+1) \times (k+1)$ rotation matrix and $R_k$ a $(k+1) \times k$ upper triangular matrix whose last row is zero. Since $Q_k$ is unitary, we have:
\begin{equation}
\label{gmres3}
J(\textbf{y}) = ||\beta \textbf{e}_1 - \bar{H}_k \textbf{y}|| = ||Q_k (\beta \textbf{e}_1 - \bar{H}_k \textbf{y})|| = ||\textbf{g}_k - R_k \textbf{y}||,
\end{equation}
where $\textbf{g}_k= Q_k \beta \textbf{e}_1$ . Since the last row of $R_k$ is zero, the minimization of \eqref{gmres3} is achieved by solving the upper triangular linear system which we have if we remuve the last row of $R_k$ and the last component of $\textbf{g}_k$. We also notice that the residual norm of the solution $\textbf{d}_k$ is equal to the $(k+1)$st component of $\textbf{g}_k$.\\
 For more details see \cite{GMRES}.\\
 
\noindent \textbf{Convergence.} As a general rule, GMRES, like others Krylov methods, performs best if the eigenvalues of $A$ are in a few tight clusters. One way to see this, keeping in mind $\textbf{d}_0=0$, is to observe the $k$th GMRES residual can be written as a polynomial in $A$ applied to the residual
\begin{equation*}
\textbf{r}_k=\textbf{b}-A\textbf{d}_k=p(A)\textbf{r}_0=p(A)\textbf{b}.
\end{equation*}
  Here $p\in \mathcal{P}_k$, this is the set of polynomial of degree $k$ with $p(0)=1$. Since the $k$th GMRES iteration satisfies 
  \begin{equation*}
  ||A\textbf{d}_k-\textbf{b} ||\leq ||A\textbf{z}-\textbf{b}||
  \end{equation*}
  for all $\textbf{z}\in\mathit{K_k}$, we must have 
  \begin{equation*}
  ||\textbf{r}_k||=\min_{p\in \mathcal{P}_k}||p(A)\textbf{r}_0 ||.
  \end{equation*}
  This simple fact can lead to very useful error estimates.
  Suppose $A$ is diagonalizable, in other words there is a nonsingular matrix $V$ such that 
   \begin{equation*}
   A=V\Lambda V^{-1}.
   \end{equation*}
   Here $\Lambda$ is a diagonal matrix with the eigenvalues of $A$ on the diagonal. If $A$ ia a diagonalizable matrix and $p$ is a polynomial, then 
   \begin{equation*}
   p(A)=Vp(\Lambda) V^{-1}.
   \end{equation*}
   \begin{theorem}
   	\label{eigen}
   	Let $A=V\Lambda V^{-1}$ be a nonsingular diagonalizable matrix. Let $\textbf{d}_k$ be the $k$th GMRES iterate. Then, for all $\bar{p_k}\in \mathcal{P}_k$,
   	\begin{equation*}
   	\frac{||\textbf{r}_k||}{||\textbf{r}_0||}\leq \mathit{k}(V) \max_{\textbf{z}\in \sigma(A)}|\bar{p_k}(\textbf{z})|.
   	\end{equation*}
   \end{theorem}
   
   Suppose, for example, that $A$ is diagonalizable, $\mathit{k}(V) = 100$, and all the eigenvalues of $A$ lies in a disk of radius 0.1 centered about 1 in the complex plane. Theorem \ref{eigen} implies (using $\bar{p_k}(\textbf{z})=(1-\textbf{z})^k$) that
   \begin{equation*}
   ||\textbf{r}_k||\leq 100(0.1)^k =0.1^{k-2}
   \end{equation*}
   Hence, GMRES will reduce the residual by a factor of, say, $10^5$ after seven iterations. And now we can see also that having clusters that are not so spread, is better. One aim of preconditioning is to change $A$ to obtain an advantageous distribution of eigenvalues.\\
   
   \subsection{Equivalences between Inexact and Quasi-Newton method} Let's consider the quasi-Newton iterate: 
   \begin{equation*}
   (F'(\textbf{x}_k) + \Delta_k) \textbf{s}_k = -F(\textbf{x}_k),
   \end{equation*}
     with $\Delta_k = B_k - F'(\textbf{x}_k)$ and $B_k$ a sequence of invertible matrices. From Theorem 1 in \cite{Emil}, we know that, if QN iterates converge to $\textbf{x}^{*}$, then they converge \textit{superlinearly} if and only if 
     \begin{equation}
     \label{QNcond}
     \frac{||(B_k-F'(\textbf{x}^*))(\textbf{x}_{k+1}-\textbf{x}_{k})||}{||\textbf{x}_{k+1}-\textbf{x}_{k}||} \rightarrow 0 \; \; as\; k\rightarrow \infty
     \end{equation} 
   The last one can be written also like this: 
      \begin{equation}
      \label{suplin QN}
      ||(F'(\textbf{x}_k)+\Delta_k -F'(\textbf{x}^*))\textbf{s}_k|| = o(||\textbf{s}_k||) \; \; as \; k \rightarrow \infty
      \end{equation}

   Now, let's consider the inexact Newton iterate: 
   \begin{equation*}
   F(\textbf{x}_k)s_k = -F(\textbf{x}_k)+\textbf{r}_k
   \end{equation*}
    The Theorem 2 in \cite{Emil} asserts that if the IN iterates converge to $\textbf{x}^*$, then this convergence is \textit{superlinear} if and only if 
    \begin{equation}
    \label{INcond}
    ||\textbf{r}_k||=o(||F(\textbf{x}_k)||) \; \; as \; k \rightarrow \infty
    \end{equation}
       We will call \eqref{QNcond} and \eqref{INcond} conditions that characterize the superlinear convergence.
   Let's write the QN iterates as IN iterates is this way
      \begin{equation*}
      F'(\textbf{x}_k)\textbf{s}_k = -F(\textbf{x}_k)-\Delta_k \textbf{s}_k,
      \end{equation*}
   and in this case \eqref{INcond} becomes 
       \begin{equation}
       \label{suplin_IN}
       ||\Delta_k\textbf{s}_k||= o (||F(\textbf{x}_k)||) \; \; as \: k \rightarrow \infty.
       \end{equation}
      In \cite{Emil} it is proved that \eqref{suplin_IN} and \eqref{suplin QN} are equivalent. \\
      
      Moreover, also the IN iterates can be written as QN iterates 
      \begin{equation*}
     (F'(\textbf{x}_k)- \frac{1}{||\textbf{s}_k||^2_2} \textbf{r}_k \textbf{s}^t_k) \textbf{s}_k= -F(\textbf{x}_k)
      \end{equation*}
       and \eqref{suplin QN} becomes 
      \begin{equation}
      \label{daINaQN}
      ||(F'(\textbf{x}_k)- \frac{1}{||\textbf{s}_k||^2_2} \textbf{r}_k \textbf{s}^t_k - F'(\textbf{x}^*))\textbf{s}_k|| = o(||\textbf{s}_k||) \; \; as \; k\rightarrow \infty
      \end{equation}
    Also in this case, in \cite{Emil} is proven that \eqref{daINaQN} and \eqref{INcond} are equivalent.
    
   The conclusion is that quasi-Newton methods and the inexact Newton methods are equivalent, in the sense that each may be used to characterize the high convergence order of the other. For example, one can use Theorem \ref{convergenza_IN} to analyze the chord method or the secant method. In the case of the chord method, the steps satisfy \eqref{inexact_newton} with $\eta_k=\mathcal{O}(||\textbf{e}_0||)$, which implies q-linear convergence if $||\textbf{e}_0||$ is sufficiently small. For the secant method, $\eta_k = \mathcal{O}(||\textbf{e}_{k-1}||)$, implying q-superlinear convergence.
       
   \section{Global convergence} 
   The convergence of Newton and inexact Newton methods is local; i.e., convergence
   is guaranteed if the initial iterate $\textbf{x}_0$ is sufficiently near a solution. Globalization techniques improve the likelihood of convergence from arbitrary starting points and most of them fall into two classes: \textbf{line search} methods and \textbf{trust-region} methods.\\
   \subsection{Line search} \label{line_search} Many times, when we are far from the solution, we don't manage to arrive to convergence, because the steps become too large or too small. In this case, with an extra condition, we decide how large to be the step in the \textbf{search direction} $\textbf{d}_k$ calculated in that iteration. So we don't update $\textbf{x}_{k+1}$ anymore in this way $\textbf{x}_{k+1} = \textbf{x}_{k} + \textbf{d}_k$, but we do as follows: $\textbf{x}_{k+1} = \textbf{x}_{k} + \lambda \textbf{d}_k$. Line search with Newton’s method is called also \textbf{damped Newton method}. \\
   One of the simplest condition imposes that the new $\textbf{x}_{k+1}$ has to make \textit{decrease} $||F||$, therefore:
   \begin{gather*}
   	\lambda = 1\\
   while \;\;\;\; ||F(\textbf{x}_{k} + \lambda \textbf{d}_k)|| \geq ||F(\textbf{x})||\\
   \lambda = \lambda /2 \\
   endwhile\\
   \textbf{x}_{k+1}=\textbf{x}_k + \lambda \textbf{d}_k
   \end{gather*}
     We call this method \textit{line search} because we search along the line segment
     \begin{equation*}
     (\textbf{x}_k, \textbf{x}_k - \textbf{d}_k)
     \end{equation*}
     to find a decrease of $||F||$. As we move from the right endpoint to the left, usually this procedure is called \textit{backtracking}.\\
    There is also another condition, that impose the new $\textbf{x}_{k+1}$ to make \textit{sufficiently decrease} $||F||$,
    \begin{gather*}
    	\lambda = 1 \; \; and \; \; \alpha \in (0,1)\\
    	while \; \;\;\;||F( \textbf{x}_{k} + \lambda \textbf{d}_k)|| \geq (1-\alpha\lambda)||F(\textbf{x})||\\
    	\lambda = \lambda /2 \\
    	endwhile\\
    	\textbf{x}_{k+1}=\textbf{x}_k + \lambda \textbf{d}_k
    \end{gather*}
   This approach is called \textbf{Armijo rule}. \\
  In these cases the factor of reduction is $\frac{1}{2}$, but sometimes a factor of 10 could be better if small values of $\lambda$ are needed for several consecutive steps. On the other hand, reducing $\lambda$ too much can be costly as well. Taking full Newton steps ensures fast local convergence. Taking \textit{as large} a fraction \textit{as
  possible} helps to move the iteration into the terminal phase in which full steps
  may be taken and fast convergence expected.\\
  Let's see different ways to choose $\lambda$.\\
  \noindent\textbf{Constant reduction.} As we saw, one possibility is to halve $\lambda$ at each try, but we can also choose other ways, like, for example, choosing a starting $\lambda_0 \in (0,1)$ and then use for each try $m$ , $\lambda = \lambda_0^m$ .\\
  \noindent\textbf{Polynomial line searches.} Choosing a reduction factor of the steplegngth that is the best for the specific step is better than constant reduction factors. If one can model the decrease
  in $||F||$ as the steplength is reduced, one might expect to be able to better
  estimate the appropriate reduction factor.
  In practice such methods usually
  perform better than constant reduction factors.\\
  If we have rejected $k$ steps, we have in hand the values
  \begin{equation*}
  ||F(\textbf{x}_k)||,||F(\textbf{x}_k+\lambda_1\textbf{d}_k)||,...,||F(\textbf{x}_k+\lambda_{k-1}\textbf{d}_k)||.
  \end{equation*}
  We can use this iteration history to model the scalar function
	  \begin{equation*}
	  f(\lambda)=||F(\textbf{x}_k + \lambda \textbf{d}_k)||^2
	  \end{equation*}
	  with a polynomial and use the minimum of that polynomial as the next steplength.
  We consider two ways that use second degree polynomials which we compute using previously computed information. After $\lambda_c$  has been rejected and a model polynomial computed, we compute the minimum $\lambda_t$ of that polynomial analytically and set
  \begin{equation}
  \lambda_+={
  	\begin{cases}
  	\label{lambda+}
  	\sigma_0\lambda_c \; \; if \; \; \lambda_t<\sigma_0\lambda_c, \\
  	\sigma_1\lambda_c \; \; if \; \; \lambda_t>\sigma_1\lambda_c,\\
  	\lambda_t \; \; \; \; otherwise
  	
  	\end{cases}}
  \end{equation}
  with $\sigma_0$ and $\sigma_1$ being safeguards avoiding to have $\lambda$ too small or large. In general, these safeguards are set to $0.1$ and $0.5$.\\
  Let's see how to calculate $\lambda_t$.\\
\noindent\textbf{Two-point parabolic model.} Here we use values of $f(0)$ and $f'(0)$ and the value of $f$ at the current $\lambda$ to construct a 2nd degree interpolating polynomial for $f$. \\
We have $f(0)= ||F(\textbf{x}_k)||^2$ and $f'(0)= 2 F(\textbf{x}_k)^T (F'(\textbf{x}_k)\textbf{d}_k)$, where $(F'(\textbf{x}_k)\textbf{d}_k)$ can be obtained by examination of the final residual of GMRES. Moreover, $f'(0)$ needs to be negative and if it doesn't happen, then we may need a new search direction. \\
Therefore the polynomial is:
\begin{equation*}
p(\lambda) = f(0) + f'(0)\lambda + c \lambda^2
\end{equation*}
with 
\begin{equation*}
c= \frac{f(\lambda_c)-f(0)-f'(0)\lambda_c}{\lambda_c^2}
\end{equation*}
Having $f'(0)<0$, so if $f(\lambda_c)>f(0)$, then $c>0$ and $p(\lambda)$ has a minimum at 
\begin{equation*}
\lambda_t = -f'(0)/(2c) > 0.
\end{equation*}
We then compute $\lambda_+$ with \eqref{lambda+}. \\
\noindent\textbf{Three-point parabolic model.}
An alternative to the two-point model, that avoids the need to approximate $f'(0)$, is a three-point model, which uses $f(0)$ and the two most recently rejected steps to create the parabola.\\
In this case we evaluates $f(0)$ and $f(1)$ and, if the full step is rejected, we set $\lambda=\sigma_1$ and try again. After rejection, we have the values 
\begin{equation*}
f(0),\; f(\lambda_c) \; \; and \; \; f(\lambda_p)
\end{equation*}
where $\lambda_c$ and $\lambda_p$ are the most recently rejected values of $\lambda$. The polynomial that interpolates $f$ at $0$, $\lambda_c$, $\lambda_p$ is 
\begin{equation*}
p(\lambda)= f(0) + \frac{\lambda}{\lambda_c - \lambda_p}\left(\frac{(\lambda-\lambda_p)(f(\lambda_c)-f(0))}{\lambda_c} + \frac{(\lambda_c-\lambda)(f(\lambda_p)-f(0))}{\lambda_p}\right)
\end{equation*}
We must consider two situations. If $p''(0)>0$, then we set $\lambda_t$ to the minimum of $p$, 
$\lambda_t= -p'(0)/p''(0)$, and apply safeguarding \eqref{lambda+} to compute $\lambda_+$. If $p''(0) \leq 0$ one could either set $\lambda_+$ to be minimum of $p$ on the interval $[\sigma_0 \lambda, \sigma_1 \lambda]$ or reject the parabolic model and simply set $\lambda_+= \sigma_1  \lambda_c$. \\


\subsection{Globalization in inexact Newton method}
As we saw, the globalization of the convergence in Newton methods is about ensuring that the step $k+1$ will reduce the norm of $F$, even through a constant less than 1, that is $||F(\textbf{x}_k+1)||< \alpha ||F(\textbf{x}_k)||$, with $0< \alpha \le 1$.
That means the direction $\textbf{s}_k$ should be a \textit{descent direction}. \\

Let's consider this minimization problem 
\begin{equation*}
\min_{\textbf{x} \in \mathbb{R}^N}{f(\textbf{x}) = \frac{1}{2} F(\textbf{x})^TF(\textbf{x})}.
\end{equation*}
A descent direction for $f$ at the current approximation $\textbf{x}$ is any vector $\textbf{p}$ such that:
\begin{equation}
\label{descendf}
\nabla f(\textbf{x})^T \textbf{p} < 0.
\end{equation}
Easily we can show that, being $\nabla f(\textbf{x})^T \textbf{p} = J(\textbf{x})^T F(\textbf{x})$, with $J(\textbf{x})= F'(\textbf{x})$, \eqref{descendf} is equal to 
\begin{equation}
\label{descendF}
F(\textbf{x})^T J(\textbf{x}) \textbf{p} < 0.
\end{equation}
For such a direction, it is shown that there exists a certain $\lambda_0 > 0$  such that $ f(\textbf{x} + \lambda \textbf{p}) < f(\textbf{x}) \; \; \forall \lambda \; : \; 0 < \lambda \le \lambda_0$. \\

If we are solving $J(\textbf{x}_{k}) \textbf{s}_k = - F(\textbf{x}_k)$ with a \textit{direct method}, so $\textbf{s}_k$ is the exact solution, because of the characteristics of Newton methods, $\textbf{s}_k$ will be always a descent direction; indeed, il we put $ \textbf{p} = - J(\textbf{x})^{-1} F(\textbf{x}) $, \eqref{descendF} is verified. This is not always true when we just approximate $\textbf{s}_k$ with an \textit{iterative method}. \\

Let's be in the case of Newton-Krylov method with GMRES and put for simplicity $F=F(\textbf{x})$ and $J= J(\textbf{x})$, consider $\bar{\textbf{s}}$ as an approximation of $J\textbf{s} = - F$. Then we can write
\begin{equation*}
F^TJ\bar{\textbf{s}}= -F^TF - F^T\bar{\textbf{r}}
\end{equation*}  
with $\bar{\textbf{r}}=-F-J\bar{\textbf{s}}$; $ \bar{\textbf{s}}$ will be the descent direction for $f$ at $\textbf{x}$ whenever $|F^T\bar{\textbf{r}}|<F^TF$, in particular, if 
\begin{equation}
\label{conddescend}
||\bar{\textbf{r}}||<||F||,
\end{equation}
 then $\bar{\textbf{d}}$ is a descent direction. \\
 
 Condition \eqref{conddescend}  means that the norm in GMRES must be reduced strictly. It holds whenever the Jacobian matrix $J$ is positive real and at least one step of GMRES is performed. But these hypothesis on $J$ are too restrictive, so a milder condition is to assume that the dimension $m$ in GMRES is large enough to ensure that the final residual is reduced by a factor of at least $\eta$, where $\eta$ is a scalar less than 1. This is how we arrive again at the expression of inexact Newton method
 \begin{equation*}
 ||F'(\textbf{x}_k)\textbf{s}+F(\textbf{x}_k)|| \leq \eta ||F(\textbf{x}_k)||.
  \end{equation*}
 \textbf{Trust region} Let $\textbf{x}$ be the current approximate solution of $F(\textbf{x})=0$. The effect of using a Krylov method to solve the Newton equations $J(\textbf{x})\textbf{d}= -F(\textbf{x})$ approximately is to take a step from $\textbf{x}$ of the form $\textbf{x} + \textbf{s}$, where $\textbf{s}$ is in the affine subspace $\textbf{d}_0 + \mathcal{K}_m$. If $V_m =[\textbf{v}_1,...,\textbf{v}_m]$ is an orthonormal basis for $\mathcal{K}_m$ and the initial guess $\textbf{d}_0 =0$, then $\textbf{d} = V_m \textbf{y}$, for some $\textbf{y} \in \mathbb{R}^m$, and we have a step of the form $\textbf{x} + V_m \textbf{y}$. \\
 As we introduced before, our global strategy will again be based upon finding a local minimum of the real-valued function $\frac{1}{2} F(\textbf{x})^TF(\textbf{x})$. Thus, we want to solve 
 \begin{equation}
 \label{minimiz_f}
 \min_{\textbf{y} \in \mathbb{R}^m}{f(\textbf{x} + V_m \textbf{y})}.
 \end{equation}
 Letting $g(\textbf{y}) = f(\textbf{x}+V_m \textbf{y})$, we have 
 \begin{equation*}
 \nabla g(\textbf{y}) = (J(\textbf{x} +V_m\textbf{y})V_m)^TF(\textbf{x}+ V_m\textbf{y})
 \end{equation*}
and, in particular, that 
 \begin{equation*}
 \nabla g(0) = (JV_m)^TF.
 \end{equation*}
If we use $F +J V_m \textbf{y}$ as a linear model of $F(\textbf{x} +V_m \textbf{y})$, then the quadratic model for $g$ is 
\begin{equation*}
\hat{g}(\textbf{y})= \frac{1}{2} ||F+JV_m\textbf{y}||^2
\end{equation*}
 Letting $B_m = V_m^TJ^TJV_m$, we have 
 \begin{equation}
 \label{ghat}
 \hat{g}(\textbf{y}) = \frac{1}{2} F^TF+F^TJV_m\textbf{y}+\frac{1}{2} \textbf{y}^TB_m\textbf{y}
 \end{equation}
 where $B_m$ is symmetric and positive semidefinite, and $\nabla \hat{g}(0) = \nabla g(0)$. If $J$ is non-singular, then $B_m$ is positive definite, since $V_m$ has orthonormal columns. The model trust region approach, that we are considering, will be based upon trying to find a solution of the problem
 \begin{equation}
 \label{minghat}
 \min_{||\textbf{y}||\le \tau}{\hat{g}(\textbf{y}), \; \; \textbf{y} \in \mathbb{R}^m}
 \end{equation}
 where $\tau$ is an estimate of the maximum length of a successful step to take from $\textbf{x}$ and, also, a measure of the size of the region in which the local quadratic model $\hat{g}(\textbf{y})$ closely agrees with the function $g(\textbf{y})$. The solution of \eqref{minghat} is in this theorem:
 \begin{theorem}
 Let $\hat{g}(\textbf{y})$ be defined by \eqref{ghat}, and assume that J is nonsingular. Then problem \eqref{minghat} is solved by
 \begin{equation*}
 \textbf{y}_m(\mu)= (B_m + \mu I)^{-1}\textbf{z}_m,
 \end{equation*}
 where $\textbf{z}_m= -\nabla\hat{g}(0)$, for the unique $\mu$ such that $||\textbf{y}_m(\mu)||=\tau$, unless $||\textbf{y}_m(0)||\le \tau$, in which case $y_m(0)= B_m^{-1}\textbf{z}_m$ is the solution.
 Furthermore, $\forall \mu \ge 0$, $\textbf{s}(\mu)= V_m\textbf{y}_m(\mu)$ defines a descend direction for $f(\textbf{x})= \frac{1}{2}F(\textbf{x})^TF(\textbf{x})$ for \textbf{x}, as long as $\textbf{z}_m \neq 0$.
 \end{theorem}
 The proof is made for Lemma 4.1 in \cite{Saad}.\\
 In the case in which $||\textbf{y}_m(0)||>\tau$, we can't determinate $\mu$ such that $||\textbf{y}_m(\mu)||=\tau$, so we solve \eqref{minghat} approximately. 
 For example, there is a dogled strategy \cite{Powell} that makes a piecewise linear approximation to the curve $\textbf{y}_m(\mu)$, and takes $\hat{\textbf{y}}_m$ as the point on this curve for which $||\hat{\textbf{y}}_m||=\tau$. We then define $\textbf{x}_{k+1}= \textbf{x}_k + \hat{\textbf{d}}$, where $\hat{\textbf{d}} = V_m \hat{\textbf{y}}_m$. If the iterate $\textbf{x}_{k+1}$ satisfied a condition like this 
 \begin{equation*}
 f(\textbf{x}+\bar{\textbf{d}})\le f(\textbf{x}) + \alpha \nabla f(\textbf{x})^T \bar{\textbf{d}}
 \end{equation*}
 with $0<\alpha<1$, we proceed to the next step, otherwise, a new value of the trust region size $\tau$ is chosen, and the procedure is repeated.\\
\newpage
\section{A new method for nonlinear problems}
\subsection{Projected Newton-Krylov method}
The framework that we are going to consider is discretized system of nonlinear reaction diffusion equations in which the solutions has to be constrained in a certain interval.\\
 A typical application was studied by van Veldhuizen, Vuik and Kleijn in \cite{before_MAIN}. They implemented the mathematical model of chemical vapor deposition that is divided in three parts: Navier-Stokes equations, energy equations and advection-diffusion equations, that model the iterations of reactive species. The last part, discretized implicitly in time in order to guarantee stability, leads to a large-scale system of strongly nonlinear algebraic equations that need to be solved in each time step. Moreover, the variables are species mass fraction, so non-negativity is required for them. The authors applied a projected Newton-Krylov method to solve the system of non linear equations of this form 
 \begin{equation*}
 \begin{cases}
 F(\textbf{x}) = 0\\ \textbf{x} \geq 0, 
 \end{cases}
 \end{equation*}
 with $ F: \mathbb{R}^n \rightarrow \mathbb{R}^n  $ continuously differentiable and with around $10^9$ unknowns. What we mean with \textit{projected Newton-Krylov method} is the is defined in the Algorithm 1. \\
\begin{algorithm}
	\caption{projected Newton-Krylov}
	\label{pNK}
	\begin{algorithmic}[1]
		\STATE 	$ \textbf{x}_0 \in [0, \infty) $, $ t \in (0,1) $, $ \eta_{max} \in [0,1) $, $ m_{max} \in \mathbb{N}  $ and $ 0 <  \lambda_{min} < \lambda_{max} < 1$
		\FOR{$k = 1, 2, \dots$ until convergence}
		\STATE Choose $\eta_k \in [0,\eta_{max}]$ and find $\textbf{d}_k$ that satisfy
		\STATE $ ||F(\textbf{x}_k) + F'(\textbf{x}_k)\textbf{d}_k||\leq \eta_k||F(\textbf{x}_k)|| $
		\STATE $ m = 0 $
		\WHILE { $m < m_{max}$}
		\STATE Choose $ \lambda \in  [\lambda_{min}, \lambda_{max}] $
		\IF{$ ||F(\mathcal{P}(\textbf{x}_k+ \lambda \textbf{d}_k))|| \leq (1 - t \lambda (1-\eta_k)) ||F(\textbf{x}_k)||$}
         \STATE	$ \textbf{x}_{k+1} = \mathcal{P}(\textbf{x}_k + \lambda \textbf{d}_k)$,  $m = m_{max} $
        \ELSE
        \STATE $ m = m + 1$
        \ENDIF
	\ENDWHILE
\ENDFOR
	\end{algorithmic}
\end{algorithm}
In line 3 the forcing term $ \eta_k $ is computed with the techniques shown in section \ref{inexact_Newton_method} and in line 7 $ \lambda $ is chosen with line search methods, for example the ones illustrated in section \ref{line_search}. And, of course, as the name suggests, it means that for the inexact Newton-method in line 4, it is used an iterative Newton-Krylov method for finding the solution $\textbf{d}_k $.
The other part of the name comes from the presence of $ \mathcal{P} $, that is a orthogonal projection on the domain, in this case $[0, \infty)$. So if we need non-negativity, we will have that the $i$th entry of $ \mathcal{P}(\textbf{x}) $ is defined by 
\begin{equation*}
\mathcal{P}_i(\textbf{x}) = \begin{cases}
x_i\;\;\;\; if\;\;\;\; x_i \geq 0\\ 0 \;\;\;\; otherwise .
\end{cases}
\end{equation*}
This is one of the simplest projection operator.
More precisely, this projection assigns to each component of $ \textbf{\textbf{x}} $, that is out of the constrains, the value of the broken restriction. So, in this case, non-negativity is prevent for all the variables of mass fraction.\\
An other important fact is that the local convergence of the Newton method is not affected, because of the non-expansiveness of the projection operator, as is stated in \textit{(c)} of Lemma \ref{lemma_grad}.

\subsection{Projected Newton-Krylov method with projected gradient direction}
\subsubsection{Idea}
  The work done in \cite{before_MAIN} shows that projected Newton-Krylov method is successful, where the classical nonlinear solver packages were not easy to use with such a big scale problem. Nevertheless, as the classical Newton method, this method does not guarantee global convergence; indeed, the search direction found by projected-Newton could not be a descent one, so could not minimize the norm of the underlying function $F$. Just to be more precise, defined $ \Theta (\textbf{x}) = \frac{1}{2} ||F(\textbf{x})||^2$, then $\textbf{d} $ is a descent direction for $\Theta (\textbf{x})  $ at point $\textbf{x}$ if  
  \begin{equation}
  \label{descentcond}
  	 \frac{d \Theta (\textbf{x} + t\textbf{d})}{dt} |_{t=0} = \nabla \Theta (\textbf{x})^T \textbf{d} < 0 .
  \end{equation}
 Here comes the main idea of the numerical method that we have studied, taken from a recent paper "Globalization technique for projected Newton-Krylov methods" published by Jinhai Chen and Cornelis Vuik \cite{MAIN}. It is known that a \textit{projected gradient direction} on a constraint set, that is $ \mathcal{P}(\textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k)) - \textbf{x}_k $, is \textit{usually} a descent direction. So, every time that the projected Newton-Krylov method is not able to find a descent direction, the algorithm will use a projected gradient one. 
 The method that comes out still keeps the advantages of the projected Newton-Krylov method, as the capacity of solving extreme large-scale problems, the matrix-free operation, and preconditioning technique, but it has also global convergence. The next step is to see how it happens. 
 
 \subsubsection{Properties of the projector operator}
 Before presenting in details this method, let's take a step back in a more general setting. Let's consider the following be a constraint system of nonlinear equations
  \begin{equation*}
  \begin{cases}
  F(\textbf{x}) = 0\\\textbf{x} \in \Omega,
  \end{cases}
  \end{equation*}
  where $\Omega$ is a convex constraint set of $\mathbb{R}^{n}$, such as $ \{\textbf{x} \in \mathbb{R} \; | \; \textbf{l} \leq \textbf{x}\leq \textbf{u}\}$, $l_i \in \{ \mathbb{R} \cup \{-\infty \}\} $ and $ u_i \in \{ \mathbb{R} \cup \{\infty\} \} $, $ l_i < u_i $ for all $ i = 1,...,n $, and $ F: \mathbb{R}^n \rightarrow \mathbb{R}^n  $ is continuously differentiable on $ \Omega $.
   
   Let recall some properties of a projection operator. 
   \begin{lem}{(see [\cite{Calamai}, Lemma 2.1])}
   	\label{lemma_grad}
   	Let $\varPi$ be the projection into $ \Theta  $.
   	
   	\begin{enumerate}[label=(\alph*)]
   		\label{propertiesP}
   		\item If $ \textbf{z} \in \Omega $ then $ (\varPi (\textbf{x}) - \textbf{x}, \textbf{z} - \varPi(\textbf{x}))\geq0 \; \; \forall \textbf{x} \in \mathbb{R}^n$ . 
   		\item $ \varPi$ is a monotone operator, that is $(\varPi(\textbf{y}) - \varPi(\textbf{x}), \textbf{y} -\textbf{x})\geq 0  $ for $ \textbf{x}, \textbf{y} \in \mathbb{R}^n $. If $ \varPi(\textbf{y}) \neq\varPi(\textbf{x}) $, then strict inequality holds. 
   		\item $ \varPi(\textbf{x}) $ is a non-expansive operator, that is, $||\varPi(\textbf{y}) - \varPi(\textbf{x})||\leq ||\textbf{y}-\textbf{x}|| $ for $ \textbf{x}, \textbf{y} \in \mathbb{R}^n$ 
   	\end{enumerate}
   \end{lem}
   The next lemma is given by Gafni and Bertsekas in [\cite{Gafni}, Lemma 3] and [ \cite{Gafni2}, Lemma 1.a] proven by Calamai and Mor\'e in [\cite{Calamai}, Lemma 2.2].
   \begin{lem}
   	\label{lem2paper}
   	Let $ \varPi $ be a projection into $\Omega  $. Given $\textbf{x} \in \mathbb{R}^n $, then function $\psi  $ defined by 
   	\begin{equation}
   	\psi (\alpha) = \frac{||\varPi(\textbf{x} + \alpha d) - \textbf{x}||}{\alpha}, \; \alpha >0,
   	\end{equation} 
   	is antitone (non-increasing).
   \end{lem}
   With these properties, we arrive to say in the next lemma that the projected Newton-Krylov method, mixed with projected gradient direction, is well-defined, that is the projected gradient direction is descent.
   \begin{lem}
   	\label{lem_descent}
   	Suppose that $ \textbf{x}_k $ is not a stationary point of $ \Theta(\textbf{x}_k) = \frac{1}{2}||F(\textbf{x}_k)||$ and $ \lambda \in (0,1] $. Then $ \mathcal{P}(\textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k)) - \textbf{x}_k $ is a descent direction for $ \Theta(\textbf{x}_k) $.
   \end{lem}
   \proof 
   Let $ \textbf{x} = \textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k)$ and $ \textbf{z}= \textbf{x}_k $ in the part (a) of Lemma~\ref{propertiesP}. An immediate consequences of the part (a) of that lemma, says that 
   \begin{align*}
   	0 & \leq \Bigl( \varPi \bigl(\textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k) \bigr) - \bigl(\textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k) \bigr), \textbf{x}_k - \varPi \bigl(\textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k)\bigr)\Bigr)\\
   	& \leq \Bigl( \bigl(\varPi(\textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k)\bigr) - \textbf{x}_k , \textbf{x}_k-\varPi \bigl(\textbf{x}_k -  \lambda \nabla \Theta (\textbf{x}_k)\bigr) \Bigr) \\
   	& \; \; \; \; + \Bigl( \lambda \nabla \Theta (\textbf{x}_k), \textbf{x}_k - \varPi \bigl(\textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k) \bigr)\Bigr)\\
   	& \leq - ||\varPi \bigl(\textbf{x}_k - \lambda \nabla \Theta(\textbf{x}_k)\bigr) - \textbf{x}_k||^2 - \lambda \nabla \Theta(\textbf{x}_k)^T \bigl(\varPi(\textbf{x}_k - \lambda \nabla \Theta (\textbf{x}_k))-\textbf{x}_k\bigr).
   \end{align*}
   So we have for our particular projection $ \mathcal{P} $
   \begin{equation}
   \label{9P}
   \nabla\Theta (\textbf{x}_k)^T \bigl(\mathcal{P}(\textbf{x}_k - \lambda \nabla \Theta(\textbf{x}_k)) - \textbf{x}_k \bigr)\leq -\frac{1}{\lambda} ||\mathcal{P}(\textbf{x}_k -\lambda \nabla \Theta (\textbf{x}_k))- \textbf{x}_k||^2 < 0.
   \end{equation}
   This indicates that the projected gradient direction is descent for the norm of $ F(\textbf{x}_k) $.
   \endproof
   Let's illustrate a particular case that we can encounter. 
   Consider a function $ F(\textbf{y}) $ of this form,
   \begin{eqnarray*}
   \begin{cases}
   F_1 (y_1,y_2)= \sqrt{2} \;\sqrt{10}  \sqrt{2 - y_1 |y_1|} ,\\
   F_2 (y_1,y_2)= \sqrt{2} \;\sqrt{10}  \sqrt{y_2^2 + (y_1 - 0.1)^2 }.
   \end{cases}
   \end{eqnarray*}
    As we can see $ F(\cdot) $ is continuously differentiable and we have $ \Theta(\textbf{y}) = 10 ( 2 -y_1|y_1| + y_2^2 + (y_1 - 0.1)^2) $. The gradient direction in (0,0) is $\textbf{d}_\textbf{y}=-\nabla \Theta (\textbf{y}) |_{\textbf{y}=(0,0)} = (2,0)^T $. Now imagine to rotate the axis $ y_1 $ e $ y_2 $ by $ 30^o $, the new coordinate system will be the one of $ (x_1, x_2) $ (see Figure \ref{vect1}).
    
\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{img1_vet}
	\caption[rotation axis]{rotation axis}
	\label{vect1}
\end{figure}

    
    
     With this coordinates, the gradient direction in $ (0,0) $ becomes: $ \textbf{d}_\textbf{x} = (2 \cos(30^o),2 \sin(30^o))^T  $.  
     Imagine also to have for the new axis this restriction $ \Omega = \{ \textbf{x}\in \mathbf{R}^2: x_1 \leq 0 \} $.\\
    Figure \ref{fig:img_gradiente_1} illustrate the plot of $ \Theta(\textbf{x}) $. We see that in $(0,0)$ the gradient direction goes out of the constrain, so we have to consider its projection (the dashed red line). The projection of the gradient direction is  descent, indeed, $ \mathcal{P}(\textbf{x} +\textbf{d}_\textbf{x}) -\textbf{x}  =  (0 ,2 \sin(30^o))^T $ and if we test with \eqref{descentcond}, we see that the projected gradient direction is descent, even if, actually, $ \Theta(\textbf{x})$ in that direction will grow, as we can see in Figure \ref{fig:img_gradiente_1}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{esempio1}
	\caption[gradient direction]{gradient direction in $ (0,0) $}
	\label{fig:img_gradiente_1}
\end{figure}

  \pagebreak
  \subsubsection{Algorithm}
  This is the expression of our algorithm. \\
  
  \begin{algorithm}
  	\caption{projected Newton-Krylov with projected gradient direction}
  	\label{pNK}
  	\begin{algorithmic}[1]
  		\STATE 	$ \textbf{x}_0 \in \Omega $, $ t, \sigma \in (0,1) $, $ \eta_{max} \in [0,1) $, $ m_{max} \in \mathbb{N}  $, $ 0 <  \lambda_{min} \leq \lambda_0 \leq \lambda_{max} < 1$ and $ FLAG_{NG} = 0 $
  		\FOR{$k = 1, 2, \dots$ until convergence}
  		\IF {$ FLAG_{NG} = 0 $ and if a preconditioned Krylov subspace method finds some $ \eta_k \in [0, \eta_{max}] $ and a vector $ \textbf{d}_k $ satisfying
  			\begin{equation}
  			||F(\textbf{x}_k) + F'(\textbf{x}_k)\textbf{d}_k||\leq \eta_k||F(\textbf{x}_k)||
  			\end{equation}
  			}
  			\STATE $ m = 0 $, $ \lambda = \lambda_{0} $ 
  		\WHILE { $m < m_{max}$}
  		\STATE Choose $ \lambda \in [\lambda_{min}, 1] $
  		\IF{
  			\begin{equation} 
  			\label{ineqPN}
  			||F(\mathcal{P}(\textbf{x}_k+ \lambda \textbf{d}_k))|| \leq (1 - t \lambda (1-\eta_k)) ||F(\textbf{x}_k)||
  			\end{equation}
  			}
  		\STATE	$ \textbf{x}_{k+1} = \mathcal{P}(\textbf{x}_k + \lambda \textbf{d}_k)$,  $m = m_{max} $, $ FLAG_{NG}= 0 $
  		\ELSE
  		\STATE $ m = m + 1$, $FLAG_{NG} = 1 $
  		\ENDIF
  		\ENDWHILE
  		\ELSE
  		\STATE $ \textbf{d}_k = - \nabla \Theta (\textbf{x}_k) $
  		\STATE $ m=0 $, $ \lambda = \lambda_0 $
  		 \WHILE { $m < m_{ma\textbf{x}}$}
  		 	\STATE Choose $ \lambda \in  (0, 1]  $
  		  	\IF{
  		  		\begin{equation}
  		  		\label{ineqPG} 
  		  		\Theta(\mathcal{P}(\textbf{x}_k+ \lambda \textbf{d}_k)) \leq \Theta (\textbf{x}_k) + \sigma \nabla \Theta(\textbf{x}_k)^T (\mathcal{P}(\textbf{x}_k + \lambda \textbf{d}_k)- \textbf{x}_k)
  		  		\end{equation}
  		  		}
  		    \STATE	$ \textbf{x}_{k+1} = \mathcal{P}(\textbf{x}_k + \lambda \textbf{d}_k)$,  $m = m_{max} $, $ FLAG_{NG}= 0 $
  		  	\ELSE
  			\STATE $ m = m + 1$
 	  		\ENDIF
 	   		\ENDWHILE
  		\ENDIF
  		\ENDFOR
  	\end{algorithmic}
  \end{algorithm}
 According to Lemma \ref{lem_descent}, if $ \textbf{x}_k $ is not a stationary point for $ \Theta (\textbf{x}_k) $, then the first part of \eqref{ineqPG}, $\Theta(\mathcal{P}(\textbf{x}_k+ \lambda \textbf{d}_k)) \leq \Theta (\textbf{x}_k)  $ should be always verified. The "task" of the second part, $ + \sigma \nabla \Theta(\textbf{x}_k)^T (\mathcal{P}(\textbf{x}_k + \lambda \textbf{d}_k)- \textbf{x}_k) $, that is always negative, is to ensure that the new point makes the norm of $ F $ smaller "enough".
    
\subsubsection{Convergence results}
The authors Chen and Vuik display in \cite{MAIN} an interesting result for the convergence of this new method, that is the content of next theorem. 
\begin{theorem}
	\label{sppaper}
	Assume that $\{\textbf{x}_k\} \subset \Omega$ is a sequence generated by the feasible projected Newton-Krylov method. Then any accumulation point of $ \{\textbf{x}_k\} $ is at least a stationary point of $ \Theta(\textbf{x}_k) $. Further, if \eqref{ineqPN} is satisfied by the projected Newton direction for all but finitely many $ k $, then $\textbf{x}^*  $ is a zero of $ F(\textbf{x}) $ on $ \Omega $.
\end{theorem}
 \proof
 Let $ \textbf{x}^* $ be an accumulation point of a sequence $ \{\textbf{x}_k\} $ generated by our method. The notation for $ \lambda $ that is used in the paper \cite{MAIN} is $ \lambda_0^{m_k} $, that indicates that  $ m_k $ is the power of $ \lambda_0 $, but to be more general we can just say that $ \lambda_0^{m_k} $ is the $ \lambda $ picked up with an arbitrary technique at the $ m_k $th attempt for the $ k $th nonlinear iteration. In addition, let's suppose that the first $\lambda  $ that we try for each nonlinear iteration $ k $ is $ \lambda_0^0 = 1 $. \\
 There are two cases.\\
 First, suppose that the projected Newton direction is used, so \eqref{ineqPN} holds, for infinitely many iterations. It follows immediately that $ F(\textbf{x}^*) = 0$ because of the local convergence properties od the projected Newton method, that are the same of the classic one. So $ \textbf{x}^* $ is a stationary point.\\
 Second, suppose that the projected gradient direction is used (\eqref{ineqPG} holds) for all but finitely many iterations.
 It follows from \eqref{ineqPG} and from Lemma \ref{lemma_grad}, that $ \{\Theta (\textbf{x}_k)\} $ is monotonically decreasing (unless the method terminates at a stationary point at any finite step) and is bounded below by 0. Hence, it converges and 
 \begin{equation*}
 \lim_{k \rightarrow \infty} ( \Theta (\textbf{x}_{k+1}) - \Theta (\textbf{x}_k) ) = 0.
 \end{equation*}
 But then, using \eqref{ineqPG},we see that 
 \begin{equation}
 \label{lim11}
 \lim_{k \rightarrow \infty} \nabla \Theta (\textbf{x}_k)^T (\mathcal{P} (\textbf{x}_k - \lambda_0^{m_k} \nabla \Theta (\textbf{x}_k))- \textbf{x}_k)=0, 
 \end{equation}
 where $ \lambda_0^{m_k} $ can be seen as the lambda chosen at step $ k $.\\
 Let $ \{ \textbf{x}_k, k \in K \} $ be a subsequence converging to $\textbf{x}^*$. We have two cases for \eqref{lim11}.
 \textit{Case 1}: Assume 
 \begin{equation*}
 \liminf_{k (\in K) \rightarrow \infty}  \lambda_0^{m_k} > 0.
 \end{equation*}
 By \eqref{lim11} and  \eqref{9P}, it follows that for dome infinite subset $ K' \subseteq K $,
 \begin{equation*}
 \lim_{k (\in K) \rightarrow \infty} - ||\mathcal{P}(\textbf{x}_k  - \lambda_0^{m_k} \nabla \Theta (\textbf{x}_k)) - \textbf{x}_k||^ 2= 0.
 \end{equation*}
  Hence, $ \textbf{x}^* $ is a stationary point of $ \Theta (\textbf{x}) $.\\
  \textit{Case 2}: Assume that there is a subsequence $ \{\textbf{x}\}_{k \in J}, J \subseteq K $ with 
  \begin{equation*}
  \lim_{k (\in J) \rightarrow \infty}  \lambda_0^{m_k} = 0.
  \end{equation*}
  So, for sufficiently large $k(\in J)$, $\textbf{x}_k$ is not a stationary point. Otherwise $ \lambda_0^{m_k} $ should be different from 0 because of \eqref{ineqPG}; in fact, if we have a stationary point, the first $ \lambda $ that we try should verify the inequality, that is $ \lambda_0^{m_k} = \lambda_0^0 = 1 $.\\
   Therefore, for sufficiently large $k(\in J) $, it holds that 
  \begin{equation}
  \label{14P}
  ||\mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k} -1} \nabla \Theta (\textbf{x}_k)) - \textbf{x}_k|| > 0,
  \end{equation} 
  because $ \textbf{x}_k $ is not a stationary point for all $ \lambda_0^{m_k} $.  In addition, it follows from \eqref{ineqPG} that 
  \begin{equation*}
  \Theta(\mathcal{P}(\textbf{x}_k+ \lambda_0^{m_{k}-1} \textbf{d}_k)) - \Theta (\textbf{x}_k) > \sigma \nabla \Theta(\textbf{x}_k)^T (\mathcal{P}(\textbf{x}_k + \lambda_0^{m_{k}-1} \textbf{d}_k)- \textbf{x}_k).
  \end{equation*}
  Moreover, by the mean value theorem, we know 
    \begin{align*}
       \Theta(\mathcal{P} & (\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k))) - \Theta (\textbf{x}_k) \\
     = & \nabla \Theta(\bm{\xi}_k)^T (\mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k))- \textbf{x}_k)\\
     = & (\nabla \Theta(\bm{\xi}_k) - \nabla \Theta(\textbf{x}_k))^T (\mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k))- \textbf{x}_k)\\
     & + \nabla \Theta(\textbf{x}_k)^T (\mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1}\nabla \Theta(\textbf{x}_k))- \textbf{x}_k) \\
      > &\sigma \nabla \Theta(\textbf{x}_k)^T (\mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k))- \textbf{x}_k),
    \end{align*}
    where $ \bm{\xi}_k $ is a point in the line segment between $ \textbf{x}_k $ and $ \mathcal{P}  (\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k)) $, so $ \bm{\xi} = \tau \textbf{x}_k + (1-\tau) \mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta (\textbf{x}_k))$ for some $\tau \in (0,1)  $.
    Consequently, 
   \begin{align*}
   	(\nabla \Theta(\bm{\xi}_k) - \nabla \Theta(\textbf{x}_k))^T & (\mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k))- \textbf{x}_k)\\
   	> &(1 -\sigma) \nabla \Theta(\textbf{x}_k)^T ( \textbf{x}_k - \mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k))).
   \end{align*}
   Further, 
\begin{equation} \label{18P}
\begin{split}
 	 \nabla \Theta(\textbf{x}_k)^T & ( \textbf{x}_k - \mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k)))\\
 	< &\frac{1}{1 -\sigma} (\nabla \Theta(\bm{\xi}_k) - \nabla \Theta(\textbf{x}_k))^T(\mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k))- \textbf{x}_k)\\
 	< &\frac{1}{1 -\sigma} ||\nabla \Theta(\bm{\xi}_k) - \nabla \Theta(\textbf{x}_k)|| ||\mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k))- \textbf{x}_k||.
 \end{split}
 \end{equation}
 From Lemma \ref{lem2paper}, we know that $ \frac{|| \textbf{x}_k - \varPi(\textbf{x}_k -  \lambda \nabla \Theta (\textbf{x}_k)) ||}{\lambda} $ is monotonically nonincreasing with respect to $ \lambda $. 
 From \eqref{9P}, we know that 
 \begin{align*}
 	\nabla \Theta(\textbf{x}_k)^T & ( \textbf{x}_k - \mathcal{P}(\textbf{x}_k - \lambda_0^{m_{k}-1} \nabla \Theta(\textbf{x}_k)))\\
 	\geq & \frac{|| \textbf{x}_k - \varPi(\textbf{x}_k -  \lambda_0^{m_k -1} \nabla \Theta (\textbf{x}_k)) ||^2}{\lambda_0^{m_k -1}}\\
 	\geq & \frac{|| \textbf{x}_k - \mathcal{P}(\textbf{x}_k -  \lambda_0 \nabla \Theta (\textbf{x}_k)) ||}{\lambda_0} || \textbf{x}_k - \mathcal{P}(\textbf{x}_k -  \lambda_0^{m_k -1} \nabla \Theta (\textbf{x}_k)) ||,
 \end{align*}
 with the assumption that $ \lambda_0 \equiv \lambda_0^1 \geq \lambda_0^{m_k-1}$. \\
   This combined with \eqref{14P} and \eqref{18P}, implies 
   \begin{equation*}
   \frac{|| \textbf{x}_k - \mathcal{P}(\textbf{x}_k -  \lambda_0 \nabla \Theta (\textbf{x}_k)) ||}{\lambda_0} < \frac{1}{1-\sigma} ||\nabla \Theta (\bm \xi_k)  - \nabla \Theta (\textbf{x}_k)||.
   \end{equation*}
   Passing to the limit as $ k(\in J)\rightarrow \infty $, we obtain 
   \begin{equation*}
      \frac{|| \mathcal{P}(\textbf{x}_* -  \lambda_0 \nabla \Theta (\textbf{x}_*)) - \textbf{x}_* ||}{\lambda_0} = 0,
   \end{equation*}
   which implies $ \textbf{x}^* $ is a stationary point.  Therefore, in either case, we establish the assertion.
 \endproof
 
 The next result is about local convergence, but first we need to recall some concepts. 
 If $ F(\textbf{x}) $ is continuously differentiable, then $ F(\textbf{x}) $ is locally Lipshitz continuous at $ \textbf{x} $, that is, there exists a constant $ L_{\textbf{x}} $ such that for all $ \textbf{y} $
 sufficiently close to $ \textbf{x} $, 
 \begin{equation*}
 ||F(\textbf{y}) - F(\textbf{x}))|| \leq L_{\textbf{x}} ||\textbf{y} - \textbf{x}||.
 \end{equation*}
 Further, if $ F'(\textbf{x}) $ is nonsingular at $ \textbf{x}^* $, so there exists a constant $ C_{\textbf{x}^*} $ such that $ ||F'(\textbf{x}^*)^{-1}|| \leq C_{\textbf{x}^*} $, then there is a costant $ C $ such that $ F'(\textbf{y})^{-1} $ exists, and 
 \begin{equation*}
 ||F'(\textbf{y})^{-1}|| \leq C
 \end{equation*}
 for all $ \textbf{y} $ sufficiently close to $ \textbf{x}^* $.
 \begin{theorem}
 	Assume that $\textbf{x}^* \in \Omega$ is a limit point of $\{\textbf{x}_k\} $ generated by the feasible projected Newton-Krylov method. Assume also that $F(\textbf{x}^*) = 0 $ and $F'(\textbf{x}^*)$ is nonsingular. Then the wole sequence $ \{\textbf{x}_k\}  $ converges to $ \textbf{x}^* $. Furthermore, for large enough $ k $, 
 	
 	\begin{enumerate}[label=(\alph*)]
 	\item if $ \eta_{max} $ and $ t $ are chosen by 
 	\begin{equation*}
 	 \begin{cases}
 	 	\eta_{max} \in (0,1),\; \; t \in [C^2L, 1), \; \; \; if  \; \;C^2L < 1,\\
 	 	\eta_{max} \in (0,\frac{1-t}{C^2L-t}),\; \; t \in (0, 1), \; \; \; if  \;	 \;C^2L \geq 1 	,
 	 \end{cases}
 \end{equation*}
 
 	where $ L $ is the Lipschitz constant of $ F $ at $ \textbf{x}^* $ and $ C $ is an upper bound of inverse of $ F(\textbf{x}) $ defined in a neighborhood of $ \textbf{x}^* $, then the projected Newton direction is eventually accepted with $ \lambda = 1$, that is, no projection gradient is carried out.
 	\item if 
 	 	\begin{equation*}
 	 	\begin{cases}
 	 	\eta_{max} \in (0,\min\{ \frac{1}{CL}, 1\}),\; \; t \in [C^2L, 1), \; \; \; if  \; \;C^2L < 1,\\
 	 	\eta_{max} \in (0,\min\{ \frac{1}{CL}, \frac{1-t}{C^2 L - t}\}),\; \; t \in (0, 1), \; \; \; if   \;C^2L \geq 1 	,
 	 	\end{cases}
 	 	\end{equation*}
 	 	then the convergence rate is Q-linear;
 	 	\item if $ \eta_k \rightarrow 0 $, the convergence rate is Q-superlinear.
 \end{enumerate}
 \end{theorem}
 \subsubsection{Previuos thoery of convergence for projected gradient direction}
 Properties of projected gradient direction have been studied since 1960 and, in particular, we propose the handling of paper \cite{Calamai}. 
 As we already know, gradient projection algorithm is defined by: 
 \begin{equation}
 \label{seqgrad}
 \textbf{x}_k = P (\textbf{x}_k - \alpha_k \nabla \Theta(\textbf{x}_k)).
 \end{equation}
 In the convergence analysis of \cite{Calamai}, there is a theorem (Theorem 2.4) that states that if $\Theta : \mathbb{R}^n\rightarrow \mathbb{R}  $ is continuously differentiable on $ \Omega $, $ \{\textbf{x}_k\} $ be the sequence generated by \eqref{seqgrad}, with a suitable choice of $ \alpha_k $ and if some subsequence $ \{x_k, k \in K\} $ is bounded, then 
 \begin{equation*}
 \lim_{k \in K, k \rightarrow \infty} \frac{||\textbf{x}_{k+1} - \textbf{x}_k||}{\alpha_k} = 0
 \end{equation*}
and any limit point of \eqref{seqgrad} is a stationary point of problem $\Theta(\textbf{x})$.
This result means it was already known that if we use only projected gradient direction, the sequence of $ \textbf{x}_k $ is going to converge in a stationary point. Theorem \ref{sppaper} adds that the mix of projected Newton directions and projected gradient directions ends still in a stationary point, moreover, if the second ones are used finitely many times, it reachs a point in which $ \Theta(\textbf{x}) = 0 $, that, actually, is our concern. \\
In general the projected gradient of $ \Theta(\textbf{x}) $ is defined by 
\begin{equation}
\nabla_{\Omega} \Theta( \textbf{x} ) \equiv argmin \{ || v + \nabla  \Theta ( \textbf{x} ) || : v \in T( \textbf{x} ) \},
\end{equation} 
where $T(\textbf{x}) $ is the \textit{tangent cone}, closure of the cone of feasible directions. In our case $ \nabla_{\Omega} \Theta (\textbf{x}_k) =  P(\textbf{x}_k - \nabla \Theta(\textbf{x}_k)) - \textbf{x}_k $. It is shown that this operator has the following properties. 
\begin{lem}
	Let $ \nabla_{\Omega} \Theta (\textbf{x}) $ be the projected gradient of $ \Theta $ at $ \textbf{x} \in \Omega$.
	\begin{enumerate}[label=(\alph*)]
	\item $ -(\nabla \Theta (\textbf{x}),\nabla_{\Omega} \Theta (\textbf{x})  )= ||\nabla_{\Omega} \Theta (\textbf{x})||^2 $.
	\item $ \min \{(\nabla \Theta (\textbf{x}),v) : v \in T(\textbf{x}), ||v|| \leq 1 \} = - || \nabla_{\Omega} \Theta (\textbf{x})||$.
	\item The point $ \textbf{x} \in \Omega $ is a stationary point of $ \Theta(\textbf{x}) $ if and only if $ \nabla_{\Omega} \Theta(\textbf{x}) = 0 $.
\end{enumerate}
\end{lem}
Point $ (b) $ is important because it is displays that $ \nabla_{\Omega} \Theta (\textbf{x}) $ is a steepest descent direction for $ \Theta $, so we can say that our projected gradient direction $  P(\textbf{x}_k - \nabla \Theta(\textbf{x}_k)) - \textbf{x}_k  $ is still the steepest one in $ \Omega $. Indeed from point $ (c) $, we can see that $ \nabla_{\Omega} \Theta (\textbf{x}_k)  $ should converge to 0 and actually this is what happens as it is stated in Theorem 3.2 of \cite{Calamai}, and more generally in Theorem 3.4.
\subsubsection{Examples}
We report one simple example that shows how projected gradient direction can "save" situations in which the projected Newton direction cannot be descent. 
Consider the following constrain problem for $ \textbf{x} = (x_1, x_2)^T \in \Omega = (-\infty, 1] \times (-\infty, 1] $:
\begin{equation*}
\begin{cases}
x_1^2 - x_2 -2 = 0,\\
x_1 - x_2 = 0,
\end{cases}
\end{equation*}
with $\textbf{x}^* = (-1,-1)^T  $ unique solution. We have :
\begin{eqnarray*}
F'(\textbf{x}) = \begin{pmatrix}
	2x_1 & -1 \\
	1 & -1
\end{pmatrix}&,\\
\nabla \Theta (\textbf{x}) = (2x_1(x_1^2-x_2-2)+x_1-x_2,& -(x_1^2-x_2-2)-(x_1-x_2))^T.
\end{eqnarray*}
If we take $ \textbf{x}_0 = (1, \frac{1}{2})^T $, then it follows that the Newton direction is $ \textbf{d}_0 = (2, \frac{5}{2})^T $ with $ \eta_0 = 0 $ in Algorithm \ref{pNK}. As we know from theory, this direction, if calculated exactly, is a descent one; indeed:
\begin{equation*}
\nabla \Theta (\textbf{x}_0)^T\textbf{d}_0 = \left(-\frac{5}{2}, 1\right) \cdot \left(2, \frac{5}{2}\right)^T = - \frac{5}{2} < 0.
\end{equation*}
The problem comes when we project it because we find
\begin{eqnarray*}
 P (\textbf{x}_0 +\lambda \textbf{d}_0) - \textbf{x}_0 &= \left( \min\{1+2\lambda, 1 \},\min \{1, \frac{1}{2} + \frac{5\lambda}{2}\} \right)^T - \left(1, \frac{1}{2}\right)^T \\
 &=\left(0, \min \{\frac{1}{2}, \frac{5\lambda}{2} \}\right),
\end{eqnarray*}
and therefore
\begin{eqnarray*}
\nabla \Theta (\textbf{x}_0)^T (P (\textbf{x}_0 +\lambda \textbf{d}_0) - \textbf{x}_0) &= \left(-\frac{5}{2}, 1\right) \cdot \left(0, \min \{\frac{1}{2}, \frac{5\lambda}{2}\}\right)^T\\
&= \min\{ \frac{1}{2}, \frac{5 \lambda}{2} \} \geq 0,
\end{eqnarray*}
for any $ \lambda \in (0,1] $. We conclude that the projected Newton direction is not a descent one, but, as we expect from Lemma \ref{lem_descent}, the projected gradient direction is a good one; indeed:
\begin{eqnarray*}
	P (\textbf{x}_0 -\lambda \nabla \Theta (\textbf{x}_0)) - \textbf{x}_0 &= \left( \min\{1+\frac{5}{2}\lambda, 1 \},\min \{1, \frac{1}{2} - \lambda\} \right)^T - \left(1, \frac{1}{2}\right)^T \\
	&=\left(0,-\lambda \right)^T,
\end{eqnarray*}
and
\begin{equation*}
	\nabla \Theta (\textbf{x}_0)^T (P (\textbf{x}_0 -\lambda \nabla \Theta (\textbf{x}_0)) - \textbf{x}_0) = \left(-\frac{5}{2}, 1\right) \cdot \left(0,-\lambda \right)^T = - \lambda < 0.
\end{equation*}
The geometrical scenario is illustrated in Figure \ref{esempio7}, where we see that the projected Newton direction cannot be descent and that the projected gradient is still descent, even if is not the steepest. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{example7}
	\caption[gradient direction]{blue Newton direction (projected and not), green is gradient direction (projected and not)}
	\label{esempio7}
\end{figure}

In addiction to the analytical example that we have just shown, there is also a numerical one in \cite{MAIN} (Example 8), whose aim is to show the performances of the new projected Newton-Krylov method. It is about a system of nonlinear equations for $ \textbf{x} = (x_1, \dots, x_n)^T $ with $ \Omega = \{ \textbf{x} \in \mathbb{R}^n | x_1 \in [0.8, 2], x_i \in [0.5, 2], 2 \leq i \leq n \} $. The system is
\begin{equation*}
\begin{cases}
x_1^2 - 1 = 0,\\
x_1 - x_2^3 = 0,\\
x_2 - x_3^3 = 0, \\
\; \;  \; \; \vdots \\
x_{n-2} - x_{n-1}^2 = 0,\\ 
x_{n-1} - x_n = 0
\end{cases}
\end{equation*}
with solution  $ \textbf{x}^* = (1, \dots, 1)^T $ in $ \Omega $. 
As a first approach, we tried to implement this problem by our self with $ n = 100 $ in order to compare our results with the ones of the paper. The information that we had about settings of Algorithm \ref{pNK}, was the following: 
 	\begin{enumerate}[label=(\alph*)]
 		\item initial guess for the nonlinear iterations $ \textbf{x}_0 (1:20) = 0.9  $ and $ \textbf{x}_0 (21:100) = 0.5$, and zero vector for the linear ones;
 		\item termination tolerance rule for nonlinear iteration: $ ||F(\textbf{x}_k)|| \leq 10^{-12} $;
 		\item choice of linear search parameter $ \lambda = \lambda_0^ m$, with $ \lambda_0 = 0.5 $ for porjected Newton direction and $ \lambda_0 = 0.8 $ for projected gradient one.
 		\item $ \sigma = t = 10^{-4} $, $ m_{max} = 20 $, $ \eta_{max} = 0.9 $;
 		\item linear solver GMRES.
 	\end{enumerate}
 	For the choice of forcing term $ \eta_k $ the authors just say that they used one of the options in \cite{Forcingterm}, the ones that we illustrated in the previous chapter. The problem for us was that we did not know exactly which one was the choice used and which was the value of the parameters. Using all information we had and the values of residual norm $ ||F(\textbf{x})|| $ that we should have obtained (table in Figure \ref{results_example8}), we applied a reverse iterative technique to find out the forcing terms. In practice, we interpolated the tolerance needed in the linear solver (so the forcing term) to arrive to the residual norms of Figure \ref{results_example8} and, after few steps, we found that they used Choice 2 \eqref{Choice2} with $ \alpha = 2 $, $ \gamma = 0.9 $ and $ \eta_0 \simeq 0.765518 $.\\
 	\begin{figure}[h]
 		\centering
 		\includegraphics[width=0.6\linewidth]{results_example8}
 		\caption[gradient direction]{Table of results of Example 8 in \cite{MAIN}, PN indicates projected Newton direction and PG projected gradient direction}
 		\label{results_example8}
 	\end{figure}
 	First of all, we have to point out that in Figure \ref*{results_example8}, each value of $ ||F(\textbf{x})|| $ is actually the input value of that iteration, so, for example, $ 3.4873 $ corresponds to $ ||F(\textbf{x}_0)|| $, and the value in the second iteration is the residual norm that came from the first iteration. The same is true for values of $ \lambda $ , so the columns of $ \lambda $ and $ ||F(\textbf{x})|| $ are shifted by one iteration respect to the other two columns.\\
 After all this procedure, we realized that the values of $ \lambda $ in iterate 5 and 6 are reported wrongly, indeed in 5 $ \lambda $ is equal to $ 0.5^2 $ and not to $ 0.8^2 $ because it refers to the previous step that uses PN, and not PG. Indeed in 6, $ \lambda$ is equal to $0.8 $ and not to $ 0.5 $ for the same logic. This correction needs to be done for iterates 9, 10, 13 and 14 as well. We have reported our results in Figure \ref{our_example8} in the same format as in Figure \ref{res_paper}.\\
 An other thing that we noticed, more important, was that, in order to have the same results as in the paper, we had to force $ \lambda $ to be equal to $ 0.8 $ each time PG was used, even if \eqref{ineqPG} was not verified. When we did the test as Algorithm \ref{pNK} indicates, at iteration 5, $ \lambda = 0.8 $ was not enough, so the algorithm selected $ \lambda = 0.8^4 $ to satisfy \eqref{ineqPG}. Therefore, all the convergence behavior, that follows step 5, changes. Figure \ref{res_paper} and \ref{res_our}, shown their and our trend of the residual norm.\\
 	\begin{figure}[h]
 		\centering
 		\includegraphics[width=1\linewidth]{ourresults8}
 		\caption[gradient direction]{Table of our results for Example 8 in \cite{MAIN}}
 		\label{our_example8}
 	\end{figure}
 	 	\begin{figure}[h]
 	 		\centering
 	 		\includegraphics[width=0.8\linewidth]{res_paper}
 	 		\caption[gradient direction]{Residual norms according to paper's result, blu point are PN and red ones are PG}
 	 		\label{res_paper}
 	 	\end{figure}
 	 	\begin{figure}[h]
 	 		\centering
 	 		\includegraphics[width=0.8\linewidth]{res_our}
 	 		\caption[gradient direction]{Residual norms according to our results after the correction}
 	 		\label{res_our}
 	 	\end{figure}
 	 Actually, there was no need to implement all by our self to see that there is something wrong. In fact, if we notice the trend of the residual norm in Figure \ref{results_example8} and \ref{res_paper}, we realize that is not non-increasing. That should not happen when \eqref{ineqPG} is verified, since $ \sigma $ has to be positive and $\nabla \Theta(\textbf{x}_k)^T (\mathcal{P}(\textbf{x}_k + \lambda \textbf{d}_k)- \textbf{x}_k)  $ has to be negative because the projected gradient direction is a descent one (Lemma \ref{lem_descent}). So the residual norm has to be strictly decreasing. \\
 	 We notice that in our implementation much more iterations were required to reach convergence and for most of them it is used projected gradient direction, so the decrease of residual norm is very slow. Indeed the use of PG cannot be considered as a massive one because it is too slow, it should intervene in isolated steps when PN is not able to find a acceptable direction. By the way, if we implement the algorithm without PG, we see that it is not going to converge because the PN forces $\textbf{x} $ to arrive to the other solution $ \textbf{x}^* = (-1, \dots, -1)^T$, that is out of $ \Omega $, so $ \textbf{x} + \lambda \textbf{d} $ is always projected on the constrains, consequently the algorithm is a kind of stuck. 
 	
  \newpage
\begin{thebibliography} {99}
	
	\bibitem {Kelley1} C.T.Kelley. Solving Nonlinear Equations with Newton's Method.  \textit{SIAM Fundamentals of Algorithms, Philadelphia, 2003.}
	
	
	\bibitem {Kelley2} C.T.Kelley. Iterative Methods for Linear
	and Nonlinear Equations. \textit{SIAM Philadelphia, 1995.}
	
	
	\bibitem {Forcingterm} Eisenstat SC, Walker HF. Choosing the forcing terms in an inexact Newton method. \textit{SIAM Journal on Scientific Computing 1996; 17 : 16-32.}
	
	\bibitem {GMRES} Saad Y, Schultz MH. GMRES: a generalized minimal residual method for solving nonsymmetric linear systems. \textit{SIAM Journal on Scientific and Statistical Computing 1986; 7 : 856-869.}
	
	\bibitem {Emil} Emil C\v{a}tinas. The inexact, inexact perturbed,and quasi-newton methods are equivalent models. \textit{Mathematics of computation, March 23, 2004; Volume 74, Number 249, Pages 291-301.}	
	
	\bibitem {Saad} Peter N. Brown, Youcef Saad: Hybrid Krylov methods for nonlinear systems of equations. \textit{SIAM, J. Sci. and Stat. Comput., May, 1990; Volume 11, Number 3, Pages 450-481.}	
	
	\bibitem {Powell} M. J. D. Powell, A hybrid method for nonlinear equations.\textit{P. Rabinowitz, ed., Numerical Methods for Nonlinear Equations, Gordon-Breach, New York, 1970.}

   \bibitem{MAIN} Chen, Jinhai, and Cornelis Vuik. "Globalization technique for projected Newton–Krylov methods." \textit{International Journal for Numerical Methods in Engineering} 110.7 (2017): 661-674.
	
	\bibitem{before_MAIN}van Veldhuizen, S., Cornelis Vuik, and Chris R. Kleijn. "On projected Newton–Krylov solvers for instationary laminar reacting gas flows."\textit{ Journal of Computational Physics} 229.5 (2010): 1724-1738.
	
	\bibitem{Calamai}Calamai, Paul H., and Jorge J. Moré. "Projected gradient methods for linearly constrained problems." \textit{Mathematical programming} 39.1 (1987): 93-116.
	
	\bibitem{Gafni}Gafni, Eli M., and Dimitri P. Bertsekas. "Convergence of a gradient projection method." (1982).
	
	\bibitem{Gafni2}Gafni, Eli M., and Dimitri P. Bertsekas. "Two-metric projection methods for constrained optimization." \textit{SIAM Journal on Control and Optimization} 22.6 (1984): 936-964.
	
	\bibitem{Schubert}Schubert, L. K. "Modification of a quasi-Newton method for nonlinear equations with a sparse Jacobian." \textit{Mathematics of Computation} 24.109 (1970): 27-30.
	\end{thebibliography}

\end{document}
